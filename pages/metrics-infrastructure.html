Metrics infrastructure — SBAB Docs  documentation SBAB Docs Ansible Controller Ansible Roles Ansible Vault Argo Workflows Authentication & Authorization Bitbucket Boost Chatbot Booli Ceph Dex Fortigate troubleshooting Generate certificate Go GitHub Copilot GitLab @Sbab User Guide GitLab Maintenance Grafana Istio Java Jenkins Jenkinsfile Jmeter Kafka Maintenance Kafka @Sbab User Guide Kafka Connect Kubectl Access Kubernetes Lab environment Local Open Web Metrics infrastructure Local/in-cluster Prometheus Central Prometheus Grafana MongoDB Netbox NVIDIA Neo4J Sonatype Nexus repository Oracle OWASP Database OWASP @Sbab User Guide Pact Principles of Security Prometheus ReactJS Redis Renovate S3 Security Guidelines for Developers Sentry Maintenance Sentry User Guide Configuring Variables for Services Deployed to Kubernetes Software Architecture SonarQube Maintenance SonarQube User Guide Structurizr System Landscape Vagrant Zipkin Windows Pipelines SBAB Docs » Metrics infrastructure View page source Metrics infrastructure ï This document describes how the metrics infrastructure: Prometheus, Thanos Query and Grafana - is set up. Local/in-cluster Prometheus ï All Kubernetes clusters runs an instance of the kube-prometheus stack, the Prometheus instance is configured with short term retention and is not accessed by end users. The stack is installed with Ansible role k8s_cluster_metrics_prometheus . This Prometheus instance is then scraped by the central Prometheus instances with long term retention. For troubleshooting purposes it is possible to access the Grafana and Prometheus instances from this stack at: grafana-internal-ki.sys.sbab.se prometheus-internal-ki.sys.sbab.se Obviously replace âkiâ and âsysâ with whatever data center and environment you want. Central Prometheus ï In the oscommon Kubernetes clusters ki and uv we have the following namespaces hosting the central Prometheus instances with long-term retention: metrics-sys metrics-acc metrics-stage metrics-prod The Helm chart deploying Prometheus and Thanos Query in these namespaces is at TNT/promstack . Here is what one of these namespaces looks like: $ kubectl -n metrics-sys get pods
NAME                                      READY   STATUS    RESTARTS   AGE
grafana-54c977df9-pscjb                   1/1     Running   0          22h
prometheus-promstack-0                    3/3     Running   0          49d
thanos-query-promstack-67b7685789-ggh57   1/1     Running   0          48d
thanos-query-promstack-67b7685789-vj8xt   1/1     Running   0          48d The Prometheus instances are configured to scrape all metrics from the Prometheus instances with short term retention. So Prometheus in the metrics-sys namespace in the oscommon K8S clusters will scrape: k8sproxy-ki.sys.sbab.se:8080 k8sproxy-uv.sys.sbab.se:8080 Grafana ï End users access the Grafana instance in the metrics-<env> namespace when they go to for instance: grafana.sys.sbab.se . Grafana deployment is configured at AFTO/grafana . Grafana is configured with Thanos Query as the default data source. Thanos Query is used to deduplicate data from Prometheus. Thanos Query asks Prometheus in both data centers for data. So if Prometheus for sys in oscommon Kista is down, Grafana should still work as it will get the data from oscommon Upplands-VÃ¤sby. Previous Next © Copyright 2024, TNT. Built with Sphinx using a theme provided by Read the Docs .