Kafka @Sbab User Guide — SBAB Docs  documentation SBAB Docs Ansible Controller Ansible Roles Ansible Vault Argo Workflows Authentication & Authorization Bitbucket Boost Chatbot Booli Ceph Dex Fortigate troubleshooting Generate certificate Go GitHub Copilot GitLab @Sbab User Guide GitLab Maintenance Grafana Istio Java Jenkins Jenkinsfile Jmeter Kafka Maintenance Kafka @Sbab User Guide Local Kafka & Zipkin setup Configuration Spring Boot Dropwizard Local configuration Without authenticated SSL With authenticated SSL Avro Format Schema Registry Wire format Subject Name Strategy Compatibility Checks Addresses Maven Plugin Configuration Schema Registry API Reference Configuration Auto Schema Registration Client Configuration Client code Consumer Producer Examples Migration guide kafka-ui kafkactl kcat Get consumer groups ACL - secure topics Default access Adding ACL rules for services Auth in micro services Kafka Connect Kubectl Access Kubernetes Lab environment Local Open Web Metrics infrastructure MongoDB Netbox NVIDIA Neo4J Sonatype Nexus repository Oracle OWASP Database OWASP @Sbab User Guide Pact Principles of Security Prometheus ReactJS Redis Renovate S3 Security Guidelines for Developers Sentry Maintenance Sentry User Guide Configuring Variables for Services Deployed to Kubernetes Software Architecture SonarQube Maintenance SonarQube User Guide Structurizr System Landscape Vagrant Zipkin Windows Pipelines SBAB Docs » Kafka @Sbab User Guide View page source Kafka @Sbab User Guide ï Local Kafka & Zipkin setup ï TNT provides a kafka-all-in-one repo containing a docker-compose.yml file with a complete local setup of Kafka and Zipkin. The following components are included: Zookeeper Broker Schema registry Connect Control center Ksqldb server Ksqldb cli Ksql datagen Rest proxy Zipkin The local README.md file contains additional
information to get started. Configuration ï Spring Boot ï Set the following properties: spring : kafka : bootstrap-servers : ${KAFKA_NODES_AUTH} properties : security : protocol : ${KAFKA_SECURITY} sasl : mechanism : ${KAFKA_SASL_MECHANISM} jaas : config : org.apache.kafka.common.security.scram.ScramLoginModule required username="${KAFKA_USERNAME}" password="${KAFKA_PASSWORD}"; Do not use custom code to create kafka templates bean(s) for spring, for example: @Configuration @EnableKafka public class KafkaProducerConfiguration { @Value ( "${spring.kafka.bootstrap-servers}" ) private String bootstrapServers ; private Map < String , Object > producerProperties () { Map < String , Object > props = new HashMap <> (); props . put ( ProducerConfig . BOOTSTRAP_SERVERS_CONFIG , bootstrapServers ); props . put ( ProducerConfig . KEY_SERIALIZER_CLASS_CONFIG , StringSerializer . class ); props . put ( ProducerConfig . VALUE_SERIALIZER_CLASS_CONFIG , ByteArraySerializer . class ); return props ; } @Bean public KafkaTemplate kafkaTemplate () { Map < String , Object > configProperties = producerProperties (); ProducerFactory producerFactory = new DefaultKafkaProducerFactory ( configProperties ); return new KafkaTemplate ( producerFactory ); } } This will override the default kafkaTemplate bean created by spring boot and it will not include the new properties
for security.protocol , sasl.mechanism and sasl.jaas.config as specified in the application.yml file. Dropwizard ï Set the following properties: kafka : bootstrap-servers : ${KAFKA_NODES_AUTH} properties : security : protocol : ${KAFKA_SECURITY} sasl : mechanism : ${KAFKA_SASL_MECHANISM} jaas : config : org.apache.kafka.common.security.scram.ScramLoginModule required username="${KAFKA_USERNAME}" password="${KAFKA_PASSWORD}"; Local configuration ï Without authenticated SSL ï Set the following properties for sys: kafka : bootstrap-servers : t-sth6-kaf01-l.sbab.se:9092,t-sth2-kaf03-l.sbab.se:9092 properties : security : protocol : PLAINTEXT Set the following properties for acc: kafka : bootstrap-servers : kazoo11.acc.sbab.se:9092,kazoo21.acc.sbab.se:9092,kazoo12.acc.sbab.se:9092 properties : security : protocol : PLAINTEXT Set the following properties for stage: kafka : bootstrap-servers : s-sth6-kaf01-l.sbab.se:9092,s-sth2-kaf03-l.sbab.se:9092,kazoo31.stage.sbab.se:9092 properties : security : protocol : PLAINTEXT With authenticated SSL ï Set the following properties for sys: kafka : bootstrap-servers : t-sth6-kaf01-l.sbab.se:9097,t-sth2-kaf03-l.sbab.se:9097 properties : security : protocol : SASL_SSL sasl : mechanism : SCRAM-SHA-256 jaas : config : org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="K@fk@Admin"; ssl : endpoint : identification : algorithm : "" Set the following properties for acc: kafka : bootstrap-servers : kazoo11.acc.sbab.se:9097,kazoo12.acc.sbab.se:9097 properties : security : protocol : SASL_SSL sasl : mechanism : SCRAM-SHA-256 jaas : config : org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="K@fk@Admin"; ssl : endpoint : identification : algorithm : "" Set the following properties for stage: kafka : bootstrap-servers : s-sth6-kaf01-l.sbab.se:9097,s-sth2-kaf03-l.sbab.se:9097,kazoo31.stage.sbab.se:9097 properties : security : protocol : SASL_SSL sasl : mechanism : SCRAM-SHA-256 jaas : config : org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="K@fk@Admin"; ssl : endpoint : identification : algorithm : "" Ask TNT for the cacerts file and place it under ${JAVA_HOME}/lib/security/cacerts . Avro Format ï TNT highly recommends using Avro as the data serialization format for
messages published to Kafka. The Kafka Project in Gitlab is used to host the
repositories that contains the Avro schema files. In order generate the avro schema library, you should inherit from
the kafka-topics-parent artifact and include the avro-maven-plugin . Example pom.xml file: <project xmlns= "http://maven.apache.org/POM/4.0.0" xmlns:xsi= "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation= "http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd" > <modelVersion> 4.0.0 </modelVersion> <parent> <groupId> se.sbab.kafka.topics </groupId> <artifactId> kafka-topics-parent </artifactId> <version> 1.0.6 </version> </parent> <artifactId> tnt-info </artifactId> <version> 1.0-SNAPSHOT </version> <packaging> jar </packaging> <name> tnt-info </name> <build> <plugins> <plugin> <groupId> org.apache.avro </groupId> <artifactId> avro-maven-plugin </artifactId> </plugin> </plugins> </build> </project> All *.avsc schema files should be located at the project root level.
The TNT kafka-utils library can be used to serialize
and deserialize avro formatted messages. Tip The avro-maven-plugin can be configured to handle custom conversions, such as the UUID format. Example in credit-onboarding-events using a java.time.Instant for createdAt and a java.util.UUID for loanApplicationId . { "namespace" : "se.sbab.kafka.event" , "type" : "record" , "name" : "LoanApplicationAcceptedEvent" , "doc" : "This event is emitted when a received loan application is accepted for further processing" , "fields" : [ { "name" : "createdAt" , "type" : { "type" : "long" , "logicalType" : "timestamp-millis" } }, { "name" : "loanApplicationId" , "type" : { "type" : "string" , "logicalType" : "uuid" } } ] } Warning The kafka-utils library is deprecated and should be replaced by the kafka schema-registry Java code: public class EventUtils { public static byte [] convert ( SpecificRecord event ) { // ... } public static < T extends SpecificRecord > T convert ( byte [] event , Schema schema ) { // This method will handle both the old raw byte array format and the new schema-registry format } } Note The latest version 1.0.20 of the kafka-utils library will transparently handle deserialization of kafka events serialized using
the kafka-schema-registry format during migration to the kafka schema registry. Schema Registry ï Wire format ï The Kafka Schema Registry provides a service
for storing and retrieving the schema format used for kafka messages. Compared to the raw avro format, the schema registry wire format adds an additional 5 extra bytes to the message. Bytes Area Description 0 Magic Byte Confluent serialization format version number; currently always 0. 1-4 Schema ID 4-byte schema ID as returned by Schema Registry. 5-â¦ Data Serialized data for the specified schema format (Avro). Subject Name Strategy ï A serializer registers a schema in Schema Registry under a subject name, which defines a namespace in the registry: Compatibility checks are per subject Versions are tied to subjects When schemas evolve, they are still associated to the same subject but get a new schema ID and version The subject name depends on the subject naming strategy . Strategy Description Use when TopicNameStrategy Derives subject name from topic name (default) One record type will be written to one specific topic RecordNameStrategy Derives subject name from record name One record type will be written to different topics TopicRecordNameStrategy Derives the subject name from topic and record name Different record types will be written to one topic Note The full class names for the above strategies consist of the strategy name prefixed by io.confluent.kafka.serializers.subject. Compatibility Checks ï For Schema Evolution and Compatibility , the following compatibility levels can be defined for all schema formats: Compatibility Type Changes allowed Check against which schemas Upgrade first BACKWARD Delete fields Add optional fields Last version Consumers BACKWARD_TRANSITIVE Delete fields Add optional fields All previous versions Consumers FORWARD Add fields Delete optional fields Last version Producers FORWARD_TRANSITIVE Add fields Delete optional fields All previous versions Producers FULL Add optional fields Delete optional fields Last version Any order FULL_TRANSITIVE Add optional fields Delete optional fields All previous versions Any order NONE All changes are accepted Compatibility checking disabled Depends Note Default subject name strategy is TopicNameStrategy and default compatibility type is BACKWARD Warning In order fo the above compatibility checks to work, both the schema used to write and read the record must be present.
This does NOT work as expected with the raw kafka-utils EventUtils.convert() byte array format since the schema used to write
the record is normally not provided. Addresses ï The schema registry setup at SBAB are provided at the following locations: Environment Address Sys https://schema-registry.sys.sbab.se Acc https://schema-registry.acc.sbab.se Stage https://schema-registry.stage.sbab.se Prod https://schema-registry.prod.sbab.se Local dev http://localhost:8081 Unit test mock://dummy.address.sbab.se Note The correct schema-registry url is set for all services in Kubernetes using the environment variable KAFKA_SCHEMA_REGISTRY_URL .
See Client Configuration for an example application.yml file. Note Local development setup using confluent docker compose file .
Unit testing using a mock:// address will create a local in memory schema registry. Maven Plugin ï TNT provides a custom kafka-schema-registry maven plugin, fully integrated with our build pipeline. This plugin is used to register schemas in the schema registry,
validate compatibility requirements and set compatibility type. The following maven goals are available: Goal Purpose kafka-schema-registry:test-compatibility Read schemas from the local file system and test for compatibility against the Schema Registry server(s). kafka-schema-registry:validate Read schemas from the local file system and validate them locally, before registering them. kafka-schema-registry:register Read schemas from the local file system and register them on the target Schema Registry server(s) Note Default schema-registry-url is https://schema-registry.sys.sbab.se .
Use the command line property -DaltSchemaRegistryUrls=http://localhost:8081 for local schema registry. Configuration ï For the normal case where only one record type is written to a single topic ( TopicNameStrategy ) using the default BACKWARD compatibility type, the default configuration may be used: <build> <plugins> <plugin> <groupId> org.apache.avro </groupId> <artifactId> avro-maven-plugin </artifactId> </plugin> <plugin> <groupId> se.sbab.mojo </groupId> <artifactId> kafka-schema-registry-maven-plugin </artifactId> <version> 1.0.4 </version> <configuration> <subjects> <subject> <topic> tnt.message </topic> <records> <record> TntInfoEvent.avsc </record> </records> </subject> </subjects> </configuration> <executions> <execution> <phase> test </phase> <goals> <goal> validate </goal> <goal> test-compatibility </goal> </goals> </execution> </executions> </plugin> </plugins> </build> A more complete example, where different records are written to a single topic ( TopicRecordNameStrategy )  can be
found in the credit-onboarding-events repo: <configuration> <imports> <import> LoanPurpose.avsc </import> </imports> <subjects> <subject> <strategy> TopicRecordNameStrategy </strategy> <compatibility> BACKWARD_TRANSITIVE </compatibility> <topic> credit-onboarding </topic> <records> <record> LoanApplicationAcceptedEvent.avsc </record> <record> LoanApplicationApprovedEvent.avsc </record> <record> LoanApplicationDeniedEvent.avsc </record> <record> PersonalLoanApplicationReceivedEvent.avsc </record> <record> PartIdAssignedEvent.avsc </record> <record> ApplicantAddedEvent.avsc </record> <record> CreditReportFetchedEvent.avsc </record> <record> CreditRulePolicyEvaluatedEvent.avsc </record> <record> YellowLightTurnedOffEvent.avsc </record> </records> </subject> </subjects> </configuration> Schema Registry API Reference ï Use curl to interact with the schema-registry api : View all the subjects registered in the schema registry: curl -- silent - X GET https : // schema - registry . sys . sbab . se / subjects / | jq . Output: [ "credit-onboarding-se.sbab.kafka.event.PersonalLoanApplicationReceivedEvent" , "credit-onboarding-se.sbab.kafka.event.CreditReportFetchedEvent" , "credit-onboarding-se.sbab.kafka.event.LoanApplicationRejectedEvent" , "credit-onboarding-se.sbab.kafka.event.DocumentLinkedToComplementEvent" , "credit-onboarding-se.sbab.kafka.event.ApplicantAddedEvent" , "credit-onboarding-se.sbab.kafka.event.PartIdAssignedEvent" , "credit-onboarding-se.sbab.kafka.event.LoanApplicationDataSetEvent" , "tnt.message-value" , "credit-onboarding-se.sbab.kafka.event.LoanApplicationApprovedEvent" , "credit-onboarding-se.sbab.kafka.event.CreditRulePolicyEvaluatedEvent" , "credit-onboarding-se.sbab.kafka.event.LoanApplicationAcceptedEvent" ] View latest version of schema for subject credit-onboarding-se.sbab.kafka.event.LoanApplicationApprovedEvent : curl -- silent - X GET https : // schema - registry . sys . sbab . se / subjects / credit - onboarding - se . sbab . kafka . event . LoanApplicationApprovedEvent / versions / latest | jq . Output: { "subject" : "credit-onboarding-se.sbab.kafka.event.LoanApplicationApprovedEvent" , "version" : 1 , "id" : 34 , "schema" : "{\"type\":\"record\",\"name\":\"LoanApplicationApprovedEvent\",\"namespace\":\"se.sbab.kafka.event\",\"doc\":\"This event is emitted when a loan application is approved\",\"fields\":[{\"name\":\"createdAt\",\"type\":{\"type\":\"long\",\"logicalType\":\"timestamp-millis\"}},{\"name\":\"createdBy\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"},\"doc\":\"The user (human or system) that has approved the loan application\"},{\"name\":\"loanApplicationId\",\"type\":{\"type\":\"string\",\"logicalType\":\"uuid\"}}]}" } If you want to make breaking schema changes (only allowed during development), the following curl commands may need to be used: curl -- silent - X DELETE https : // schema - registry . sys . sbab . se / subjects / my - topic - se . sbab . kafka . event . MyEvent | jq curl -- silent - X DELETE https : // schema - registry . acc . sbab . se / subjects / my - topic - se . sbab . kafka . event . MyEvent | jq curl -- silent - X DELETE https : // schema - registry . stage . sbab . se / subjects / my - topic - se . sbab . kafka . event . MyEvent | jq curl -- silent - X DELETE https : // schema - registry . prod . sbab . se / subjects / my - topic - se . sbab . kafka . event . MyEvent | jq More examples can be found in the schema-registry tutorial Configuration ï Auto Schema Registration ï By default, client applications automatically register new schemas. If they produce new messages to a new topic,
then they will automatically try to register new schemas. This is very convenient in development environments,
but in production environments we recommend that client applications do not automatically register new schemas. Note Within the application, you should disable automatic schema registration by setting the configuration parameter auto.register.schemas=false Client Configuration ï Spring Boot client config using the schema registry should be similar to: spring : kafka : client-id : your-client-id-here bootstrap-servers : ${KAFKA_NODES_AUTH} consumer : group-id : your-group-id-here key-deserializer : org.springframework.kafka.support.serializer.ErrorHandlingDeserializer value-deserializer : org.springframework.kafka.support.serializer.ErrorHandlingDeserializer producer : key-serializer : org.apache.kafka.common.serialization.StringSerializer value-serializer : io.confluent.kafka.serializers.KafkaAvroSerializer properties : spring.deserializer.key.delegate.class : org.apache.kafka.common.serialization.StringDeserializer spring.deserializer.value.delegate.class : io.confluent.kafka.serializers.KafkaAvroDeserializer schema : registry : url : ${KAFKA_SCHEMA_REGISTRY_URL:https://schema-registry.sys.sbab.se} auto.register.schemas : false specific.avro.reader : true security : protocol : ${KAFKA_SECURITY} sasl : mechanism : ${KAFKA_SASL_MECHANISM} jaas : config : org.apache.kafka.common.security.scram.ScramLoginModule required username="${KAFKA_USERNAME}" password="${KAFKA_PASSWORD}"; Note specific.avro.reader is needed to deserialize to a SpecificRecord instead of the default GenericRecord Python client ( confluent-kafka-python ) config using environment variables class Consumer ( DeserializingConsumer ): def __init__ ( self ): sbab_ca_path = env ( 'SBAB_CA_CERT' , default = get_absolute_filepath ( 'sbab-ca.crt' )) schema_registry_client = SchemaRegistryClient ( { 'url' : env ( 'KAFKA_SCHEMA_REGISTRY_URL' , default = 'https://schema-registry.sys.sbab.se' ), 'ssl.ca.location' : sbab_ca_path , } ) string_deserializer = StringDeserializer ( 'utf_8' ) avro_deserializer = AvroDeserializer ( schema_registry_client = schema_registry_client , from_dict = Greeting . dict_to_greeting ) DeserializingConsumer . __init__ ( self , conf = { 'bootstrap.servers' : env ( 'KAFKA_NODES_AUTH' , default = 's-sth6-kaf01-l.sbab.se:9097,s-sth2-kaf03-l.sbab.se:9097' ), 'group.id' : env ( 'KAFKA_CLIENT_GROUP_ID' , default = 'group-python' ), 'security.protocol' : env ( 'KAFKA_SECURITY' , default = 'SASL_SSL' ), 'sasl.mechanism' : env ( 'KAFKA_SASL_MECHANISM' , default = 'SCRAM-SHA-256' ), 'sasl.username' : env ( 'KAFKA_USERNAME' , default = 'admin' ), 'sasl.password' : env ( 'KAFKA_PASSWORD' , default = 'K@fk@Admin' ), 'ssl.ca.location' : sbab_ca_path , 'key.deserializer' : string_deserializer , 'value.deserializer' : avro_deserializer , 'default.topic.config' : { 'auto.offset.reset' : 'smallest' }, }) Client code ï Consumer ï When using the schema-registry together with the specific.avro.reader: true configuration, the kafka receiver will
directly accept instances of the correct event class, without the need to convert between byte[] and specific event
types. Before: @Component public class KafkaReceiver { private static final Logger LOGGER = LoggerFactory . getLogger ( KafkaReceiver . class ); @KafkaListener ( topics = "${kafka.avro.topic}" ) public void receive ( ConsumerRecord < String , byte []> record ) { TntInfoEvent message = EventUtils . convert ( record . value (), TntInfoEvent . getClassSchema ()); LOGGER . info ( "received key='{}' message='{}'" , record . key (), message ); } } After: @Component public class KafkaReceiver { private static final Logger LOGGER = LoggerFactory . getLogger ( KafkaReceiver . class ); @KafkaListener ( topics = "${kafka.avro.topic}" ) public void receive ( ConsumerRecord < String , TntInfoEvent > record ) { LOGGER . info ( "received key='{}' message='{}'" , record . key (), record . value ()); } } If the kafka listener can receive events of different types, for instance when using the TopicRecordNameStrategy ,
then the value type should be a of the base type org.apache.avro.specific.SpecificRecord . The type received will
still be the correct event type. Tip If the receiver accepts the generic SpecificRecord type, you can use the spring org.springframework.context.ApplicationEventPublisher to publish events and an @EventListener to handle the
correct event type. @Component class LoanApplicationCommandEventListenerService ( private val applicationEventPublisher : ApplicationEventPublisher ) { @StreamListener ( target = "loanApplicationServiceCommandsChannel" ) fun listen ( event : SpecificRecord ) { applicationEventPublisher . publishEvent ( event ) } } @Service class LoanApplicationEventListenerService ( private val loanApplicationAggregateService : LoanApplicationAggregateService ) { @EventListener fun on ( event : TurnOffYellowLightCommandEvent ) { loanApplicationAggregateService . apply ( event . toCommand ()) } } Producer ï The kafka producer should also be updated to use specific event types: Before: @Autowired public KafkaSender ( KafkaTemplate < String , byte []> kafkaTemplate ) { this . kafkaTemplate = kafkaTemplate ; } After: @Autowired public KafkaSender ( KafkaTemplate < String , TntInfoEvent > kafkaTemplate ) { this . kafkaTemplate = kafkaTemplate ; } Examples ï Code using the schema-registry at SBAB can be found here: Kotlin code with a kafka stream listener: loan-application-view-service TNT Spring Boot example: spring-template TNT Python example: python-template Migration guide ï In order to migrate from using the kafka-utils library EventUtils.convert() methods to the schema-registry format,
the following steps should be performed: Add the kafka-schema-registry-maven-plugin to the Kafka event repository. Example: tnt-info . Merge the event repository to master in order to register the event(s) in the schema registry and get a released
version of the library. In Gitlab, a new version of the schema will be pushed to Nexus and registered in the schema registries
once a new git tag is created. Update the kafka-utils version in the consumer(s) to the latest version (>= 1.0.20). This version will transparently
handle deserialization of messages sent with either EventUtils.convert() or the schema-registry with no
additional code changes needed. Update all kafka-producer(s) for the topic to use the schema-registry. Update all kafka-consumer(s) for the topic to use the schema-registry. Note Each step above should be released to production before the next step can be performed. The migration should be performed per topic. If a consumer service listens to multiple topics using different formats
(i.e. schema-registry Avro format and raw byte array Avro format), then the kafka consumer configuration may need to be
moved from the application.yml / application.properties file to separate ConsumerFactory beans. @Configuration @EnableKafka public class KafkaConfiguration { @Autowired private KafkaProperties kafkaProperties ; @Bean public LoggingErrorHandler errorHandler () { return new LoggingErrorHandler (); } @Bean public ConsumerFactory < String , byte []> consumerFactoryRaw () { Map < String , Object > configProps = new HashMap <> ( kafkaProperties . buildConsumerProperties ()); configProps . put ( ErrorHandlingDeserializer . KEY_DESERIALIZER_CLASS , StringDeserializer . class ); configProps . put ( ErrorHandlingDeserializer . VALUE_DESERIALIZER_CLASS , ByteArrayDeserializer . class ); return new DefaultKafkaConsumerFactory <> ( configProps ); } @Bean public ConcurrentKafkaListenerContainerFactory < String , byte []> kafkaListenerContainerFactoryRaw () { ConcurrentKafkaListenerContainerFactory < String , byte []> factory = new ConcurrentKafkaListenerContainerFactory <> (); factory . setConsumerFactory ( consumerFactoryRaw ()); return factory ; } @Bean public ConsumerFactory < String , TntInfoEvent > consumerFactorySchemaRegistry () { Map < String , Object > configProps = new HashMap <> ( kafkaProperties . buildConsumerProperties ()); configProps . put ( ErrorHandlingDeserializer . KEY_DESERIALIZER_CLASS , StringDeserializer . class ); configProps . put ( ErrorHandlingDeserializer . VALUE_DESERIALIZER_CLASS , KafkaAvroDeserializer . class ); return new DefaultKafkaConsumerFactory <> ( configProps ); } @Bean public ConcurrentKafkaListenerContainerFactory < String , TntInfoEvent > kafkaListenerContainerFactorySchemaRegistry () { ConcurrentKafkaListenerContainerFactory < String , TntInfoEvent > factory = new ConcurrentKafkaListenerContainerFactory <> (); factory . setConsumerFactory ( consumerFactorySchemaRegistry ()); return factory ; } } @Component public class KafkaReceiver { private static final Logger LOGGER = LoggerFactory . getLogger ( KafkaReceiver . class ); @KafkaListener ( topics = "${kafka.avro.topic}" , containerFactory = "kafkaListenerContainerFactoryRaw" , groupId = "groupid1" ) public void receiveRaw ( ConsumerRecord < String , byte []> record ) { TntInfoEvent message = EventUtils . convert ( record . value (), TntInfoEvent . getClassSchema ()); LOGGER . info ( "received schema-registry event as raw byte array: key='{}' message='{}'" , record . key (), message ); } @KafkaListener ( topics = "${kafka.avro.topic}" , containerFactory = "kafkaListenerContainerFactorySchemaRegistry" , groupId = "groupid2" ) public void receiveTntInfo ( ConsumerRecord < String , TntInfoEvent > record ) { LOGGER . info ( "received schema-registry event: key='{}' message='{}'" , record . key (), record . value ()); } } application.yml: spring : kafka : client-id : tnt-2 bootstrap-servers : ${KAFKA_NODES_AUTH} consumer : auto-offset-reset : earliest key-deserializer : org.springframework.kafka.support.serializer.ErrorHandlingDeserializer value-deserializer : org.springframework.kafka.support.serializer.ErrorHandlingDeserializer producer : key-serializer : org.apache.kafka.common.serialization.StringSerializer value-serializer : io.confluent.kafka.serializers.KafkaAvroSerializer properties : schema : registry : url : ${KAFKA_SCHEMA_REGISTRY_URL:https://schema-registry.sys.sbab.se} auto.register.schemas : false specific.avro.reader : true security : protocol : ${KAFKA_SECURITY} sasl : mechanism : ${KAFKA_SASL_MECHANISM} jaas : config : org.apache.kafka.common.security.scram.ScramLoginModule required username="${KAFKA_USERNAME}" password="${KAFKA_PASSWORD}"; The receiveRaw method can handle both the raw byte array Avro format and the new schema-registry Avro format.
The receiveTntInfo can only handle messages sent using the Avro schema-registry format but will handler schema
format changes using the configured compatibility rules. The spring-template migration branch contains a working example of a client that can receive messages using two different formats. Note It is also possible to remove the ErrorHandlingDeserializer consumer config from the application.yml file and instead use ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG and ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG in the KafkaConfiguration class: @Bean public ConsumerFactory < String , byte []> consumerFactoryRaw () { Map < String , Object > configProps = new HashMap <> ( kafkaProperties . buildConsumerProperties ()); configProps . put ( ConsumerConfig . KEY_DESERIALIZER_CLASS_CONFIG , StringDeserializer . class ); configProps . put ( ConsumerConfig . VALUE_DESERIALIZER_CLASS_CONFIG , ByteArrayDeserializer . class ); return new DefaultKafkaConsumerFactory <> ( configProps ); } ... @Bean public ConsumerFactory < String , TntInfoEvent > consumerFactorySchemaRegistry () { Map < String , Object > configProps = new HashMap <> ( kafkaProperties . buildConsumerProperties ()); configProps . put ( ConsumerConfig . KEY_DESERIALIZER_CLASS_CONFIG , StringDeserializer . class ); configProps . put ( ConsumerConfig . VALUE_DESERIALIZER_CLASS_CONFIG , KafkaAvroDeserializer . class ); return new DefaultKafkaConsumerFactory <> ( configProps ); } kafka-ui ï In the test-environments you can use the tool kafka-ui for visualization/testing. You can find it here: kafka-ui.sbab.se kafkactl ï We are using a fork of kafkactl on our SQL jump hosts, which only has read access to Kafka in production. kcat ï kcat (formerly kafkacat) is a command-line
utility that you can use to test and debug Kafka deployments. You can use kcat to produce, consume, and list topic
and partition information for Kafka. Described as ânetcat for Kafkaâ, it is a swiss-army knife of tools for inspecting
and creating data in Kafka. In consumer mode, kcat reads messages from a topic and partition and prints them to standard output. This example
lists all messages from Kafka in sys on topic tnt-message produced using Avro format with the schema-registry: kcat - b kazoo11 . sys . sbab . se : 9092 - t tnt . message - s value = avro - r https : // schema - registry . sys . sbab . se Note The Java version used by kcat must have the SBAB root certificate installed in order to use https to access the
schema-registry. Get consumer groups ï To list available consumer groups of a topic, in this example tnt.message , run: kafkactl get consumer - groups -- topic tnt . message ACL - secure topics ï TNT is working on introducing ACL rules for Kafka access.
This means that there will be possible to get topics with secure access, controlled by a project in Gitlab .
The repository will contain ACL rules for services running in K8S. Default access ï By default services are provided with Kafka user user credentials, for a user named admin .
This has been the case, even before ACL were introduced. The admin user will not change, and it will have Read and Write access to all topics,
except for the topics specified in the kafka/classified-topics repo . Adding ACL rules for services ï If you are in need of secure/classified topics, you will need to add specifications for this in the kafka/classified-topics repo .
To do this, please follow the steps below carefully: Either clone the kafka/classified-topics repo , or use the Gitlab
editor to create the specifications for your service. Make a Merge request for your changes. If your specification file does not follow the correct format (See below), this should get caught in the test step of
the build pipeline. When merged, your changes will be applied to Kafka in all environments. You will have to redeploy your service, after your ACL rules are merged and deployed, for your service to
get the correct user credentials to access Kafka with access to the topics specified in the kafka/classified-topics repo . In kafka/classified-topics repo there is a folder named acl , this
is where you will need to add your ACL specifications for the secure/classified topics. Letâs say you have a service named super-loan-calculator-service , which needs Read access to the topic super-loan-events ,
Write access to calculated-super-loans and Read/Write access to super-duper-loan . You will then need to add a file named acl/super-loan-calculator-service.yml . Note You will have to use the .yml extension, as .yaml is not included by the pipeline. The file should then look like this: --- - topic : super-loan-events access : - consumer - topic : calculated-super-loans access : - producer - topic : super-duper-loan access : - consumer - producer Note A JSON Schema is included in the repo for your IDE to be able to give you some hints and validation
of the YAML file. If youâre using IntelliJ, there is a config provided in .idea/jsonSchemas.xml adding
the JSON Schema for all .yml files in the acl directory. Auth in micro services ï Note This is currently only supported for Services inside OpenStack (City Networks) . Warning ${KAFKA_NODES} is deprecated and will be removed the 3rd of June 2019. To implement authentication and authorization for Kafka communication and topics in a micro service, follow these steps: Previous Next © Copyright 2024, TNT. Built with Sphinx using a theme provided by Read the Docs .