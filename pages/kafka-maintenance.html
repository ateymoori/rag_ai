Kafka Maintenance — SBAB Docs  documentation SBAB Docs Ansible Controller Ansible Roles Ansible Vault Argo Workflows Authentication & Authorization Bitbucket Boost Chatbot Booli Ceph Dex Fortigate troubleshooting Generate certificate Go GitHub Copilot GitLab @Sbab User Guide GitLab Maintenance Grafana Istio Java Jenkins Jenkinsfile Jmeter Kafka Maintenance Kafka + Zookeeper Playbook Server configuration Authentication and authorization JAAS / SASL and SCRAM ACL Kafka ports / listeners Client configuration Services outside of OpenStack (City Networks) Services inside OpenStack (City Networks) Services in Kubernetes Common configuration options Bootstrap server Security SASL Mechanism Restart Kafka Upgrade 1. ZooKeeper upgrade 2. Kafka upgrade Certificate renewal Reset offset of Kafka consumer group (to-datetime) Reset offset for a Kafka connect connector (to beginning) Kafka NSX Server Kafka @Sbab User Guide Kafka Connect Kubectl Access Kubernetes Lab environment Local Open Web Metrics infrastructure MongoDB Netbox NVIDIA Neo4J Sonatype Nexus repository Oracle OWASP Database OWASP @Sbab User Guide Pact Principles of Security Prometheus ReactJS Redis Renovate S3 Security Guidelines for Developers Sentry Maintenance Sentry User Guide Configuring Variables for Services Deployed to Kubernetes Software Architecture SonarQube Maintenance SonarQube User Guide Structurizr System Landscape Vagrant Zipkin Windows Pipelines SBAB Docs » Kafka Maintenance View page source Kafka Maintenance ï Kafka + Zookeeper Playbook ï Run this from the Ansible Controller: Note In the example ansible-playbook run below, make sure to replace ${TARGET_ENV} with
the environment you want to deploy to. Should be one of [oslab, ossys, osacc, osstage, osprod]
By setting this variable is_kazoo Kafka and Zookeeper services will be stopped if reboot is needed in yum_update role. ansible-playbook --inventory inventories/ ${ TARGET_ENV } playbooks/deploy-kazoo.yml -e is_kazoo = true Server configuration ï Kafka brokers are currently configured using 5 separate ports for different purpose of
communication. You can read some details about this below, but in-depth details are
available in the Ansible Kafka Role . Authentication and authorization ï The TNT Team is planning to implement authentication and authorization for Kafka communication
and topics. This plan will be implemented over a longer time, so that changes will have as minimal
impact on services as possible. But services will need to be prepared, and configuration
changes will have to be made, to be able to continue using Kafka in the future. JAAS / SASL and SCRAM ï The JAAS configuration file is currently quite simple, as it is only for authentication
over one single port. SASL Mechanism is set to be SCRAM-SHA-256 as this should be sufficient for internal
authentication, and is a tiny bit more secure over PLAINTEXT ports than just PLAIN auth. ACL ï In the future we will implement ACLâs to control the access on Kafka Topics,
this will mean that the login credentials will be different for most of our services. Kafka ports / listeners ï Current state of the ports configured for Kafka is listed in the table below name description number extplain Plaintext port for external communication, no authentication. TO BE REPLACED! 9092 intplain Plaintext port for inner-swarm communication, no authentication 9093 interbroker SSL port for communication between Kafka brokers, no authentication 9094 extssl SSL port for external communication, no authentication 9095 intauthed Plaintext port for inner-swarm communication, authenticated with SASL_SCRAM 9096 extauthed SSL port for external communication, authenticated with SASL_SCRAM 9097 The future goal is however to have internal communication in Kubernetes on port 9092 with SASL authentication,
as 9092 is the standard PLAINTEXT port number for Kafka. Port 9093 will in the future be the external SASL
authenticated SSL port. Below is a table displaying our future goal port configuration: name description number intauthed Plaintext port for inner-swarm communication, authenticated with SASL_SCRAM 9092 extauthed SSL port for external communication, authenticated with SASL_SCRAM 9093 interbroker SSL port for communication between Kafka brokers, authenticated with SASL_SCRAM 9094 Client configuration ï To configure your micro service / application (from now on referenced as client), you will need some information about
our internal Kafka setup. Below you will find the details you need to configure your client. Services outside of OpenStack (City Networks) ï For example IS, which is using Kafka, is hosted in Basefarm. For these applications and services we need to use a
separate Kafka port, for security reasons. In our case, the secure port (SSL) is 9097 .
This means that the bootstrap servers for applications external to OpenStack should currently be something like this: stage: s-sth6-kaf01-l.sbab.se:9097,s-sth2-kaf03-l.sbab.se:9097,kazoo31.stage.sbab.se:9097
other envs: kazoo11.${env}.sbab.se:9097,kazoo12.${env}.sbab.se:9097,kazoo21.${env}.sbab.se:9097 Kafka is undergoing a migration hence the stage environment is a bit different. This will be updated again once all environments
have been migrated. Services inside OpenStack (City Networks) ï All services inside Kubernetes should use the environment variables provided with Kafka configuration.
For more information, see below in Services in Kubernetes . If you are intending to deploy services or applications that are not orchestrated using Kubernetes, then you will
most probably be using Ansible to deploy your service. All the configurations that are provided in Kubernetes, through
environment variables, are also provided in Ansible as inventory facts. Services in Kubernetes ï When you are deploying your services in Kubernetes, you should have configured your application/service so that it
will read the environment variables provided to it in the Kubernetes service deployment. This is the only way that you can
make sure that your configuration will always be up to date with the Kafka Server configurations. variable name description KAFKA_NODES The bootstrap server string
to be used in your config
files. KAFKA_SECURITY Security type, used for
encryption and authentication
of your client to the server.
eg. SSL or SASL_PLAINTEXT KAFKA_SASL_MECHANISM The type of SASL auth used
for authenticating your client
eg. PLAIN or SCRAM-SHA-256 If you want to know more about the authentication and authorization options of Kafka, please refer
to the Kafka documentation provided by https://www.confluent.io . Common configuration options ï For all Kafka clients (producer/consumer) there are some common options that needs to be specified, here are some
explanation about these options. Bootstrap server ï While configuring Kafka you will first of all need to know the servers to connect to. This is usually a string with a
comma-separated list of Kafka brokers and the port to connect to. For example like the environment variable we provide
our services with inside Kubernetes. Example stage: KAFKA_NODES=s-sth6-kaf01-l.sbab.se:9093,s-sth2-kaf03-l.sbab.se:9093,kazoo31.stage.sbab.se:9093 . As we have several ports to connect to, the services and applications outside of Kubernetes will have to configure this
for themselves in the separate environments. The following paragraphs will explain what is needed. Security ï This is where we specify if the communication should be PLAINTEXT or SSL . If authentication
is needed, SSL_SASL will be specified, which means SSL encrypted traffic, with SASL authentication activated. SASL Mechanism ï Here we specify the SASL Authentication type. PLAIN is similar to the HTTP Basic auth, where you will just provide
username and password for being able to connect. SCRAM , which we will use is a bit more complex as it uses
SHA strong hashes with salt and iterations provided. Please read more about it in the Kafka auth docs . In our Kafka clusters the SASL Mechanism is set to SCRAM-SHA-256 as this should be sufficient for internal
authentication, and is a tiny bit more secure over PLAINTEXT ports than just PLAIN auth. Restart Kafka ï ansible-playbook --inventory inventories/os<env> playbooks/deploy-kafka.yml --extra-vars stop_service = true --extra-vars start_service = true -t stop_kafka,start_kafka Upgrade ï 1. ZooKeeper upgrade ï Locate the ZooKeeper version in: https://zookeeper.apache.org/releases.html Update zk_version in inventories/os<env>/group_vars/kafka.yml Run: ansible - playbook -- inventory inventories / os < env > playbooks / deploy - zookeeper . yml Verify that /opt/zookeeper is symlinked to the new version by running this on e.g. kazoo11 : ls - la / opt / zookeeper List topics: / opt / kafka / bin / kafka - topics . sh -- zookeeper localhost : 2181 -- list 2. Kafka upgrade ï Monitor progress in Grafana: https://grafana.<env>.sbab.se/d/JOkbWI2Wk/kafka-overview?orgId = 1 & refresh = 30s Locate the Kafka and Scala versions in: https://kafka.apache.org/downloads , e.g: Kafka: 3.3.1 Scala: 2.13 Update kafka_version and kafka_scala_version in inventories/os<env>/group_vars/kafka.yml Run: ansible - playbook -- inventory inventories / os < env > playbooks / kafka / deploy . yml Re-run the same playbook if the Ensure Kafka tar is downloaded locally task fails but the tar is added in Nexus Verify that /opt/kafka is symlinked to the new version by running this on e.g. kazoo11 : ls - la / opt / kafka List topics: / opt / kafka / bin / kafka - topics . sh -- bootstrap - server localhost : 9092 -- list Verify the version in Splunk: âKafka version: <version>â sourcetype=âkafka_lab.osâ Update broker_protocol_version in inventories/os<env>/group_vars/kafka.yml The version is the major.minor version of the kafka_version , e.g 3.3 in this example Run the playbook and verify the topics Update message_format_version in inventories/os<env>/group_vars/kafka.yml The version is the major.minor version of the kafka_version , e.g 3.3 in this example Run the playbook and verify the topics Certificate renewal ï Run the Makefile-Kafka in Docker, see the repository README.md Update kafka.server.keystore.jks.orig with the newly generated file, for each environment Run: ansible - playbook -- inventory inventories / os < env > playbooks / deploy - kafka . yml Reset offset of Kafka consumer group (to-datetime) ï Login to the kazoo server Modify group_name , date_time and topic_name in the example below. Format for date_time is 2022-10-04T17:00:00.000 (local time 19:00:00). Run e.g.: /opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group <group_name> --reset-offsets --to-datetime <date_time> --topic <topic_name> --execute Reset offset for a Kafka connect connector (to beginning) ï Login to the kazoo server. Modify the topic_name and connector_name in the example below. Check the connect offset for the topic: /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic sbab-connect-offsets --from-beginning --property print.key = true | grep <topic_name> Example using topic cortex-loan-application-service-events : /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic sbab-connect-offsets --from-beginning --property print.key = true | grep cortex-loan-application-service-events [ "cortex-loan-application-service-events" , { "query" : "query" }] { "incrementing" :1323 } [ "cortex-loan-application-service-events" , { "query" : "query" }] { "incrementing" :1331 } [ "cortex-loan-application-service-events" , { "query" : "query" }] { "incrementing" :1346 } [ "cortex-loan-application-service-events" , { "query" : "query" }] { "incrementing" :1370 } [ "cortex-loan-application-service-events" , { "query" : "query" }] { "incrementing" :1371 } Delete the connector using this offset (the offset cannot be reset while the connector is running): curl -X DELETE --location "http://lbsys-intern.sbab.se:8888/kafka-connect/connectors/<connector_name>" Send a tombstone message message (a message with an empty value) to the sbab-connect-offsets topic using the key
from the kafka-console-consumer output above. For example: echo '["cortex-loan-application-service-events",{"query":"query"}]#' | /opt/kafka/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic sbab-connect-offsets --property "parse.key=true" --property "key.separator=#" Restart the consumer from GitLab kafka-connect-connectors pipeline. Kafka NSX Server ï For easier patching and maintenance we have added a new Kafka node kafka41 in Production and Stage. These are deployed in our NSX environment.
To redeploy this node you need to run the playbooks/vmware-nsx-provision-server.yml with extra var hostlist , since the deployment of this node is not part of the other kafka playbooks. Example deployment and installation of NSX node: ansible-playbook --inventory inventories/os<stage,prod> playbooks/vmware-nsx-provision-server.yml -e hostlist = kazoo41.stage.sbab.se
ansible-playbook --inventory inventories/os<stage,prod> playbooks/deploy-kafka.yml -e is_kazoo = true Previous Next © Copyright 2024, TNT. Built with Sphinx using a theme provided by Read the Docs .