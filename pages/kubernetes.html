Kubernetes — SBAB Docs  documentation SBAB Docs Ansible Controller Ansible Roles Ansible Vault Argo Workflows Authentication & Authorization Bitbucket Boost Chatbot Booli Ceph Dex Fortigate troubleshooting Generate certificate Go GitHub Copilot GitLab @Sbab User Guide GitLab Maintenance Grafana Istio Java Jenkins Jenkinsfile Jmeter Kafka Maintenance Kafka @Sbab User Guide Kafka Connect Kubectl Access Kubernetes Clusters Deploy cluster NVIDIA Deployment guide Activate/deactivate and remove a cluster Activate/deactivate Disable traffic to a cluster Remove cluster Adding a worker node to a cluster Add a Compute worker, without Ceph storage Add a Ceph worker Removing a worker from a cluster Remove and delete a Compute worker, without Ceph storage Remove and delete a Ceph worker Identify the OSDs to be removed Verify the capacity in your cluster after OSD removal Remove and delete the Ceph worker Disaster recovery Re-create single node Re-create entire cluster Manual commands Upgrading a cluster 2023 Strategy Things to note Preparations Execution Upgrading a cluster Considerations upgrade_k8s playbook Secret files IngressRoute Default route Custom routes Priority Rule matchers Weight Persistent storage Grow a block device Get PVC name(s) for the pod(s) Shutdown Prometheus Map PVC to Ceph image Grow the filesystem Start Prometheus External Cheat sheet Lab environment Local Open Web Metrics infrastructure MongoDB Netbox NVIDIA Neo4J Sonatype Nexus repository Oracle OWASP Database OWASP @Sbab User Guide Pact Principles of Security Prometheus ReactJS Redis Renovate S3 Security Guidelines for Developers Sentry Maintenance Sentry User Guide Configuring Variables for Services Deployed to Kubernetes Software Architecture SonarQube Maintenance SonarQube User Guide Structurizr System Landscape Vagrant Zipkin Windows Pipelines SBAB Docs » Kubernetes View page source Kubernetes ï Tip For more information, see the official Kubernetes documentation . Clusters ï TNT runs two Kubernetes clusters per environment, one in Kista and one in Upplands-VÃ¤sby. Each cluster is comprised of three server groups: masters, workers and proxies. Masters host the Kubernetes control plane and no other workloads. Workers run all the workloads (containers) except control plane. Proxies run a highly available haproxy that provides access to the Kubernetes API server and some other services. Environments: oslab, ossys, osacc, osstage, osprod, oscommon Data centers: ki, uv Deploy cluster ï There are a number of playbooks available to deploy and configure a Kubernetes cluster: Playbook Servers Purpose k8s/deploy_security_group.yml All Creates and configures the security group controlling communication between the K8S servers k8s/create_servers.yml All Create new servers from scratch k8s/configure_proxy.yml Proxies Configures the haproxy servers k8s/configure_k8s_base.yml Masters, workers Configures basic dependencies like Docker for K8S masters and workers k8s/configure_k8s_cluster.yml Masters, workers Creates a K8S cluster if one does not exist and joins masters/workers to it k8s/configure_k8s_cluster_services.yml Master (1) Installs services in Kubernetes, CNI provider, logging, metrics etc k8s/full_deployment.yml All Includes all the above, mostly used when creating a new cluster or adding new servers There also some playbooks for various maintenance tasks: Playbook Servers Purpose k8s/remove_k8s_server.yml Masters, workers Remove a K8S server from cluster and delete it k8s/upgrade_k8s.yml Masters, workers Upgrade Kubernetes to a new version Unless otherwise stated all playbooks should be run with these arguments (Ansible extra variables): ansible-playbook --inventory inventories/${ENV} playbooks/k8s/${PLAYBOOK} \
    --extra-vars primary_master=${INVENTORY_HOST_NAME} \
    --extra-vars parentgroup=k8s-${DATA_CENTER} \
    --extra-vars mastergroup=k8smaster-${DATA_CENTER} \
    --extra-vars workergroup=k8sworker-${DATA_CENTER} \
    --extra-vars proxygroup=k8sproxy-${DATA_CENTER} INVENTORY_HOST_NAME should be the name of a healthy K8S master. See Clusters for all environments and datacenters. Note If any errors regarding the task âhaproxyâ is encountered when running from
local ansible-controller, the following needs to be added to the environment table. export ANSIBLE_HASHI_VAULT_ADDR=https://vault.sbab.se export ANSIBLE_HASHI_VAULT_ROLE_ID=Role ID can be found in Vault under Common/gitlab export ANSIBLE_HASHI_VAULT_SECRET_ID=Secret ID can be found in Vault under Common/gitlab Note When deploying to the LAB environment, set the variable workergroup to k8snonmaster-${DATA_CENTER} . NVIDIA Deployment guide ï The best way to deploy a GPU node is to run the k8s/full_deployment.yml playbook with the normal extra vars. For the nvidia playbooks to run properly it requires a secret from our Hashicorp Vault, so make sure you run the playbooks where you can retrieve the secret with the gitlab app-role. Here is a small checklist: New node needs to have gpu in the name. Check that you have datacenter variable defined in order to retrieve the correct nvidia license file. Use a flavor with G. prefix Use a Alma Linux image. Make sure the nvidiadls network exists in the environment. If not, you need to create a Network rbac from the common project targeting your environment/project id. Run the following: openstack -- os - cloud = common1 network rbac create -- target - project cfee792785944b47926ae6e33e053e3a -- action access_as_shared -- type network c573156a - 464 c - 4885 - ab08 - d72790a0dd8b and change --os-cloud to the correct datacenter. Change --type network to nvidiadls network id in  your datacenter and change --target-project to your environment project id. Make sure your new server can reach the license servers. Check sg_nvidiadls_client security group. The rest should be taken care of with the playbooks. Activate/deactivate and remove a cluster ï Activate/deactivate ï Note (De-)Activation is for managing a list of active clusters, which jenkins should deploy to.
De-activation is done by removing your cluster from the ${ACTIVE_CLUSTER} list. Run the k8s-update-active-cluster-list.yml playbook with the following extra-var: ansible-playbook -i inventories/${ENV} playbooks/k8s-update-active-cluster-list.yml --extra-vars active_clusters="{% raw %}${ACTIVE_CLUSTERS}{% endraw %}" Where ${ACTIVE_CLUSTERS} should be replaced with a list of all the clusters you want active according to this format: ACTIVE_CLUSTERS = "{{ groups['k8smaster-ki'] | random }}:{{ groups['k8smaster-uv'] | random }}" Note The only exception to the format above is for the oscommon environment, where k8smaster should
be replaced by the string commonk8s If you use jenkins to deploy this. Use single quotes insted of dubble quotes at the beginning and end like this: -- extra - vars 'active_clusters={ % r aw %}$ {ACTIVE_CLUSTERS} { % e ndraw %}' Disable traffic to a cluster ï Follow these steps to disable incoming traffic to a cluster, in this case production: Scale down batch-runner in the cluster and wait 1 day, so that long-running batches are only scheduled on the other cluster. Disable workers from kubernetes upstream in: ext/nginx_upstream.conf_i1 ext/nginx_upstream.conf_i2 int/nginx_upstream.conf_i1 int/nginx_upstream.conf_i2 Deploy nginx/master to production Disable either ki or uv in partalen-proxy E.g. PR #1098 where the ki upstream points to uv Deploy partalen-proxy-deploy-prod PR doesnât need to be merged, if we want to revert upstreams by deploying master Migrate Dex to the other data center Migrate Argo Workflows to the other data center Verify that Splunk stops logging: Production: Nginx Kista: sourcetype=nginx-access âupstream=10.43â Nginx UV: sourcetype=nginx-access âupstream=10.44â PÃrtalen Kista: source=partalen.prod host=*ki* PÃrtalen UV: source=partalen.prod host=*uv* Argo Kista: source=âworkflow-controller.prodâ host=*ki* Argo UV: source=âworkflow-controller.prodâ host=*uv* Test environments: Nginx Kista sourcetype=nginx-access_test âupstream=10.43â Nginx UV: sourcetype=nginx-access_test âupstream=10.44â PÃ¥rtalen KI: source=partalen.ENV host=*ki* PÃ¥rtalen UV: source=partalen.ENV host=*uv* Argo KISTA: source=âworkflow-controller.ENVâ host=*ki* Argo UV: source=âworkflow-controller.ENVâ host=*uv* Toggle alerts in Grafana Disable OP5 alarms (Production only): Navigate to: https://monitor.sbab.se/monitor/index.php/search/lookup?query=ki (replace ki with uv if necessary) Click first View in listview link Tick the top left checkbox to enable bulk changes Click the Send multi action button in the top right corner Select Schedule downtime Enter a Start time in the future, e.g 1 minute in the future Enter a End time in the future Add the Jira ticket number as comment Click the Submit button, and then the Done button Verify that all hosts are scheduled for downtime Remove cluster ï Use the k8s/delete_k8s_cluster.yml playbook which remove deployments and removes servers
from OpenStack: datacenter: Which datacenter the cluster resides in (ki or uv) Playbook should be run with these arguments: ansible-playbook --inventory inventories/${ENV} playbooks/k8s/delete_k8s_cluster.yml \
  --extra-vars datacenter=${DATA_CENTER} Below an example when you want to remove deployments and removes servers
from OpenStack in KI: ansible-playbook --inventory inventories/oslab playbooks/k8s/delete_k8s_cluster.yml \ --extra-vars datacenter=ki Note If you want to re-create the complete cluster after removing it, please follow the
steps documented under Re-create entire cluster to do so. Adding a worker node to a cluster ï To add a worker node to a cluster you need to edit the hosts file for the specific environment the cluster resides in, inventories/${ENV}/hosts . Add a Compute worker, without Ceph storage ï Find the group k8sworker-${DATA_CENTER} and add the additional worker(s): [k8sworker-${DATA_CENTER}]
k8s-w1-${DATA_CENTER} os_args="{{ os_args_${DATA_CENTER} }}" floating_ip=${FLOATING_IP}
...
k8s-w${N+1}-${DATA_CENTER} os_args="{{ os_args_${DATA_CENTER} }}" floating_ip=auto Save and exit, and run the playbook playbooks/k8s/full_deployment.yml with the standard Ansible extra variables. Once completed verify that the worker has been added to the cluster: kubectl get nodes You should see output similar to the following: NAME                         STATUS   ROLES      AGE     VERSION
k8s-w1-${DATA_CENTER}        Ready    <none>     200d    v${K8S_VERSION}
...
k8s-w${N+1}-${DATA_CENTER}   Ready    <none>     100s    v${K8S_VERSION} Your cluster has now additional compute resources and will start scheduling once the status is Ready . The Openstack cluster external IP address has been automatically assigned, you should now make it permanent. Check the openstack server list to find the external IP address: openstack server list -f table -c Name -c Networks --name k8s-w${N+1}-${DATA_CENTER} The output will be something like the following: +----------------------------+--------------------------------------+
| Name                       | Networks                             |
+----------------------------+--------------------------------------+
| k8s-w${N+1}-${DATA_CENTER} | swarm=${INTERNAL_IP}, ${EXTERNAL_IP} |
+----------------------------+--------------------------------------+ Add the ${EXTERNAL_IP} to the hosts inventory file for the new worker: [k8sworker-${DATA_CENTER}]
k8s-w1-${DATA_CENTER} os_args="{{ os_args_${DATA_CENTER} }}" floating_ip=${FLOATING_IP}
...
k8s-w${N+1}-${DATA_CENTER} os_args="{{ os_args_${DATA_CENTER} }}" floating_ip=${EXTERNAL_IP} The IP address will now be assigned to the worker if it is redeployed for any reason. Add a Ceph worker ï Find the group k8sceph-${DATA_CENTER} and add the additional worker(s): [k8sceph-${DATA_CENTER}]
k8s-c1-${DATA_CENTER} os_args="{{ os_args_${DATA_CENTER} }}"
...
k8s-c${N+1}-${DATA_CENTER} os_args="{{ os_args_${DATA_CENTER} }}" Save and exit, and run the playbook playbooks/k8s/full_deployment.yml with the standard Ansible extra variables. Once completed verify that the Ceph worker has been added to the K8S cluster: kubectl get nodes You should see output similar to the following: NAME                         STATUS   ROLES      AGE     VERSION
k8s-w1-${DATA_CENTER}        Ready    <none>     200d    v${K8S_VERSION}
k8s-c1-${DATA_CENTER}        Ready    <none>     100d    v${K8S_VERSION}
...
k8s-c${N+1}-${DATA_CENTER}   Ready    <none>     100s    v${K8S_VERSION} Once the status is Ready the new worker(s) has been added to the K8S cluster. The rook operator will then add the additional OSD to the Ceph cluster, however it might take some time, at least a couple of minutes, so be patient.
You can verify that the new OSD has been added through the following command: kubectl - n rook - ceph exec - it deployment . apps / rook - ceph - tools -- ceph osd status The output should be something similar to the following: ID  HOST                              USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE
 0  k8s-c1-${DATA_CENTER}             5788k  999G      0        0       0        0   exists,up
...
 ${OSD_ID} k8s-c${N+1}-${DATA_CENTER} 400K   999G      0        0       0        0   exists,up Once the state is exists,up for your new OSD(s) available on the worker, it is completed. To troubleshoot any issues, or see the status of the rook operator you can use the following command to view the logs: kubectl -n rook-ceph logs deployment.apps/rook-ceph-operator --follow=true Removing a worker from a cluster ï There are currently two different types of workers used in our clusters. The first one is the Compute worker, they usually have no additional attached storage volumes. The second one is the Ceph worker with additional attached storage volumes, and in use in a Ceph cluster. To verify if a compute worker has additional attached storage volumes in use in a Ceph cluster, you can use the following command: kubectl - n rook - ceph exec - it deployment . apps / rook - ceph - tools -- ceph osd status From the output, you can see if the worker you wish to remove has any attached storage, and in-use storage volumes, as an OSD: ID HOST USED AVAIL WR OPS WR DATA RD OPS RD DATA STATE 0 k8s - w1 - uv 487 G 512 G 8 63.9 k 0 0 exists , up 1 k8s - w2 - uv 387 G 612 G 0 2457 0 0 exists , up 2 k8s - w3 - uv 445 G 554 G 3 23.1 k 0 0 exists , up 3 k8s - w4 - uv 377 G 622 G 0 0 0 0 exists , up Remove and delete a Compute worker, without Ceph storage ï First drain all the running load from the worker: kubectl drain ${NODE_NAME} --ignore-daemonsets --delete-emptydir-data You can verify that there are only daemonset pods running on the node: kubectl get all --all-namespaces -o wide | grep ${NODE_NAME} Note Donât worry if you have some pods left as stated, you will have some as there are daemonsets running, for example, traefik and weave.
In the case of Ceph you might have pending jobs, and pods, like the âcrash-collectorâ, wait for a couple of minutes, and if itâs still running/pending, you can just stop the deployment for âcrash-collectorâ for the corresponding node which you want to remove. Ensure that you wait until all the pending pods and jobs are finished Once the node has been drained and no more pending pods nor jobs remains, you can delete the node from the cluster: kubectl delete node ${NODE_NAME} Verify that the node is deleted: kubectl get nodes The node should now have been removed from the cluster, and you can remove it from Openstack: openstack server delete ${NODE_NAME} Remove and delete a Ceph worker ï For more information regarding Rook Ceph OSD management see the official Rook documentation . Identify the OSDs to be removed ï Before we begin, identify all the OSDs you want to remove. If you are going to remove many of them you will have to wait for rebalancing of the cluster between removing them. You can see all the OSDs attached to a node through the command: kubectl - n rook - ceph exec - it deployment . apps / rook - ceph - tools -- ceph osd tree The ID of the OSDs is in the name, osd.${OSD_ID} . Verify the capacity in your cluster after OSD removal ï Before removing anything, you will also have to be careful so that you do not run out of disk space in your cluster. Depending on the number of replications you have of each block of data, you will have to count how much storage you will require. To also ensure that your cluster does not break, now or in the future, if an OSD disappear for any reason, you have to ensure that you have enough OSDs in your cluster after the removal. To see the total number of OSDs in your cluster you can run the following command: kubectl - n rook - ceph exec - it deployment . apps / rook - ceph - tools -- ceph status | grep "osd:" The output will look something like the following, where we have 10 osds in our example: osd : 10 osds : 10 up ( since 26 h ), 10 in ( since 6 w ) To see the replication in the cluster you can use the following commands. Note You only need one of them, save the largest number of replication for use later. For the block pools: kubectl - n rook - ceph get CephBlockPool - o jsonpath = '{ ..spec.replicated.size }{" \n "}' For the object storage: kubectl - n rook - ceph get CephObjectStore - o jsonpath = '{ ..spec.dataPool.replicated.size }{" \n "}' Take the number of OSDs in your cluster, reducing it with the number of OSDs you will remove, and divide it with the replication of your cluster, this should be a whole number.
For example if you have 9 OSDs, and your replication is set to 9, you have the number 3, which is a whole number and is fine. If you do not have a whole number you might risk your cluster health if one of your OSDs disappears, or breaks for any reason. To ensure that we do not run out of storage after the removal, check how much storage you have in your cluster: kubectl - n rook - ceph exec - it deployment . apps / rook - ceph - tools -- ceph status | grep "usage:" The output will look something like the following, we have a total of 9.8 TiB, and 5.6 TiB available: usage : 4.2 TiB used , 5.6 TiB / 9.8 TiB avail Check the size of the OSDs you are going to remove: kubectl - n rook - ceph exec - it deployment . apps / rook - ceph - tools -- ceph osd df Take the size of the OSDs that you want to remove, and ensure that you are not removing too much storage. In our example we have 9.8 TiB, and we will remove one OSD with 1 TiB of storage capacity, we will have 4.6 TiB available in our cluster afterwards, which is enough. Remove and delete the Ceph worker ï Once you have identified the OSDs you need to remove, and ensuring that your cluster still has enough capacity after the removal you can continue with the task. Note If you are removing more than one OSDs, it is recommended to do the removal one by one so that the data rebalancing doesnât fail, and you risk losing data. First we have to stop the operator, otherwise it will automatically start the services we have to stop and remove: kubectl - n rook - ceph scale deployment rook - ceph - operator -- replicas = 0 After the operator is stopped we will first stop the pod related to the OSD, you got the ID from the command above: kubectl -n rook-ceph scale deployment rook-ceph-osd-${OSD_ID} --replicas=0 Once stopped we will also mark the OSD as down in the Ceph cluster, use the same ID as above: kubectl -n rook-ceph exec -it deployment.apps/rook-ceph-tools -- ceph osd down osd.${OSD_ID} We will now initiate the rebalancing of the cluster by marking the OSD as out: kubectl -n rook-ceph exec -it deployment.apps/rook-ceph-tools -- ceph osd out osd.${OSD_ID} The cluster will now begin to to rebalance, you can follow the progress through the command: kubectl - n rook - ceph exec - it deployment . apps / rook - ceph - tools -- ceph status You have to wait for the rebalance to finish, and that all your pool groups is in the state active+clean , also ensure that there are no warnings about low space. The rebalancing will take some time depending on how much storage you are using, so you will have to remain patient. Once the rebalancing is done, you can verify that all your pgs are active+clean : kubectl - n rook - ceph exec - it deployment . apps / rook - ceph - tools -- ceph status Verify that the OSD is in status down: kubectl - n rook - ceph exec - it deployment . apps / rook - ceph - tools -- ceph osd tree kubectl - n rook - ceph exec - it deployment . apps / rook - ceph - tools -- ceph status Once it is completed, you can continue to purge the OSD from the cluster: kubectl -n rook-ceph exec -it deployment.apps/rook-ceph-tools -- ceph osd purge ${OSD_ID} --yes-i-really-mean-it Verify that the OSD was removed from the CRUSH map: kubectl - n rook - ceph exec - it deployment . apps / rook - ceph - tools -- ceph osd tree It is now safe to remove the Ceph worker following the procedure in Remove and delete a Compute worker, without Ceph storage . Note When you are removing a Ceph worker you might have pending jobs, and pods, like the âcrash-collectorâ, wait for a couple of minutes, and if itâs still running, you can just stop the deployment for the corresponding node that you want to remove. Once the worker has been removed, remember to delete the old volumes as well: openstack volume delete ${NODE_VOLUME} Once you have deleted the VM and any attached volumes you will restart the Rook operator: kubectl - n rook - ceph scale deployment rook - ceph - operator -- replicas = 1 Once restarted, you are done. Disaster recovery ï Re-create single node ï First remove the node by running the k8s/remove_k8s_server.yml playbook, in addition to the usual extra variables you almost must specify: --extra-vars remove_server=${NODE_NAME} The playbook will abort early if remove_server is the same as primary_master . Then you can recreate the server by running the k8s/full_deployment.yml playbook. Re-create entire cluster ï If the entire cluster is misbehaving, after disabling traffic to the cluster, delete all servers and run the k8s/full_deployment.yml playbook.
In this example, the ki data center is re-created in oslab . Use the openstack CLI to list all existing servers: openstack -- os - cloud = lab1 server list Delete all masters, workers and proxies (might be more than the ones in this example): openstack -- os - cloud = lab1 server delete k8s - m1 - ki k8s - w1 - ki k8s - p1 - ki Run the k8s/full_deployment.yml playbook and double check that the servers are re-created: openstack -- os - cloud = lab1 server list Below a list of services can be found which should be deployed first: partner-auth-service sap-proxy transfer-validation-service transfer-service Create a branch of jenkins-pipeline and add below line to DeployAllServices.groovy playbook = playbook + " -e cluster_name=$ {DATA_CENTER} -e service_replicas=1" after this line playbook = addMissingQuotes(playbook) . Run jenkins-deploy-all-services to re-deploy all microservices with the newly created branch. Change the the replica to two in the file above and re-run jenkins-deploy-all-services job again. Manual commands ï These are useful commands, mostly used in the Deploy cluster playbook, which can be used to manually
try to recover from a disaster. View all nodes in the cluster: kubectl get nodes View all components: kubectl get componentstatuses Check health of etcd cluster (replace ${MASTER_NODE} , e.g k8s-m1-uv ): kubectl exec -it etcd-${MASTER_NODE} --namespace kube-system -- sh -c \
    "ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 \
        --cacert /etc/kubernetes/pki/etcd/ca.crt \
        --cert /etc/kubernetes/pki/etcd/server.crt \
        --key /etc/kubernetes/pki/etcd/server.key \
        endpoint health --cluster" Lists all members in the etcd cluster: etcdctl member list Remove member from the etcd cluster (find ${MEMBER_ID} in the member list): etcdctl member remove ${MEMBER_ID} Migrate pods from node and disable scheduling of new pods: kubectl drain ${NODE_NAME} --delete-local-data --ignore-daemonsets Reverts changes on node made by kubeadm init or kubeadm join : kubeadm reset -- force Remove the node from the cluster: kubectl delete node ${NODE_NAME} Upgrading a cluster 2023 ï Strategy ï Briefly explained: Disabling traffic to data center, e.g UV or KI Disabling service deployment to cluster Deleting all nodes from openstack Exporting all resources, e.g configmaps, from the cluster in the other data center Spawning new cluster with upgraded k8s version using full_deployment.yml playbook Sanity checking newly created cluster with updated k8s version such as all services running the kube-system namespace Importing resources which was exported from the other data center Enabling service deployment to service Enabling traffic to data center Things to note ï Some services may not run correctly when they have been imported to the new cluster. One of them we know to date is batch-runner which needs to be redeployed. If any other services are reporting unexpected errors it is recommended to redeploy them as per normal practice. Make sure to also delete any deployments of services that should only run in one of the data centers: dex , argo so they are not running in parallel to the other data center in the new cluster. Preparations ï Versions of the following core services needs to be updated to support the kubernetes version you are upgrading to: k8s_version https://kubernetes.io/releases/ crio_version https://github.com/cri-o/cri-o/releases cri_tools_version https://github.com/kubernetes-sigs/cri-tools/releases traefik https://github.com/traefik/traefik/releases weave https://github.com/weaveworks/weave/releases coredns https://github.com/coredns/coredns/releases These can be found under each environment under inventories/ENV/group_vars/k8s/k8s.yml Execution ï Disable traffic to a cluster, please refer to Disable traffic to a cluster Disable service deployment to cluster Example PR for disabling Example PR for enabling deployments again As can be seen in the PR for disabling, on row 18 the host group âk8smaster-uvâ should be specified on that same row if deployments to UV should be disabled. If deployments to KI is to be disabled, âk8smaster-kiâ on row 18 is to be specified instead. On row 17 environment shortname should be specified depending on what environment you are upgrading, the different shortnames for our environments are: sys acc stage prod Removing DNS entry for the cluster to be upgraded in DNS manager This is done on a Windows VM such as V-004 in DNS Manager.
In DNS Manager, navigate to the environment that is being upgraded and delete the entry for âk8sproxy-ENV BEING UPGRADEDâ Remember the IP that was set for the entry that is being deleted. Note To execute this step the following access rights are needed: DNS: Administrator (Can be applied for in ID, Gustaf Ekwall is approver) & access to the host V-004.sbab.ad is needed (Contact IT-Support) With remote app connect to 10.47.80.58 or V-004.sbab.ad login with your sa account Start dns application Connect to this server: p-dc04-w Go under the âsbab.se/ENV YOU ARE WORKING ONâ Removing all cluster nodes from openstack Does not need much explaination, can be done through Openstack web GUI or Openstack CLI Exporting resources from the other cluster in the same data center Instructions can be found in this repo: https://gitlab.sbab.se/tnt/k8s-export-config-script/-/tree/dev Spawning new cluster Running the full_deployment.yml playbook Note Example: ansible-playbook --inventory inventories/oslab playbooks/k8s/full_deployment.yml --extra-vars primary_master=k8s-m1-ki --extra-vars parentgroup=k8s-ki --extra-vars mastergroup=k8smaster-ki --extra-vars workergroup=k8sworker-ki --extra-vars proxygroup=k8sproxy-ki Warning REVIEW COMMAND & CODE CAREFULLY BEFORE RUNNING! Doublechecking all versions in the group_vars folder of the environment you are working on. It is always recommended to do a first run The following ENV variables needs to be added if running on ansible controller locally: export ANSIBLE_HASHI_VAULT_ADDR=https://vault.sbab.se export ANSIBLE_HASHI_VAULT_ROLE_ID=Role ID can be found in Vault under Common/gitlab export ANSIBLE_HASHI_VAULT_SECRET_ID=Secret ID can be found in Vault under Common/gitlab Exporting resources from cluster in the other data center Instructions can be found in the following repo: https://gitlab.sbab.se/tnt/k8s-export-config-script Importing resources to the new cluster Instructions can be found in the following repo: https://gitlab.sbab.se/tnt/k8s-import-config-script Upgrading a cluster ï Considerations ï Upgrades can âskipâ patch versions but not major/minor versions - ie. 1.17 -> 1.19 = BAD, 1.17.1 -> 1.17.6 = GOOD Best practice is to first update to the latest patch release of the current K8S minor version before updating to the next minor version. During upgrade workers will be drained of containers, this will cause interruptions for Deployments with only 1 replica Note When upgrading from v1.21 to v1.22, first run the playbook clean-legacy-kubelet-flags.yml with the usual K8S extra variables. upgrade_k8s playbook ï Tip Specify the -t flag to limit which upgrade play to run, e.g. -t play3 to only upgrade the workers. Specify the --limit flag to limit which workers to run, e.g: -t play3 --limit k8s-w4-uv,k8s-w5-uv The k8s/upgrade_k8s.yml playbook implements the kubeadm upgrade procedure . Use the usual extra variables for K8S. When upgrading to a new patch version, ie. 1.17.2 -> 1.17.12 you only need to change k8s_version . When upgrading to a new minor version, ie. 1.17.12 -> 1.18.4 you need to set/change: k8s_version crio_version cri_tools_version reboot=true It takes about 10min + 1min/worker to upgrade a cluster, it can be significantly longer depending on how many containers needs to be drained on each node. You can see which version of kubelet is running by doing kubectl get nodes . Secret files ï Warning Ansible Vault is deprecated as of 2021-09-17 (but still supported). The new recommended way is to use Custom Hashicorp Vault path for variables/files in HashiCorp Vault. Follow these steps to securely add secret files in a container. In this example, we will add test-file.txt to go-template . Create inventories/ossys/group_files/go-template/test-file.txt.orig and encrypt the file. For more information, see Encrypting the secret . (optional) Create an environment variable with the path to the file, e.g in env.list : MY_SECRET_TEST_FILE_PATH =/ etc / sbab / go - template / test - file . txt Deploy go-template IngressRoute ï Default route ï By default, each service has one route enabled, which is in charge of connecting incoming requests to the service that
can handle them. Here is one example for the spring-template service: spec:
  routes:
  - kind: Rule
    match: PathPrefix(`/spring-template`)
    services:
    - name: spring-template
      port: 8080 It is possible to disable the default route by setting service_ingress_pathprefix: False in k8s_service_deploy/vars/service_name . Custom routes ï To add multiple custom routes, configure routes in k8s_service_deploy/vars/service_name . In this example, where {{ service_name }} is go-template , /go-template/api/v1/hello has the highest priority.
For 5 requests that goes to the go-template service, only 1 request goes to the go-template-2 service. /go-template/service is another rule. For 5 requests that goes to go-template-2 , 1 request goes to go-template . The default rule, /go-template has the lowest priority. All requests goes to the go-template service: routes : - match : "PathPrefix(`/{{ service_name }}/api/v1/hello`)" priority : 10 services : - name : "{{ service_name }}" port : "{{ service_port }}" weight : 5 - name : go - template - 2 port : 8080 weight : 1 - match : "PathPrefix(`/{{ service_name }}/service`)" priority : 10 services : - name : "{{ service_name }}" port : "{{ service_port }}" weight : 1 - name : go - template - 2 port : 8080 weight : 5 - match : "PathPrefix(`/{{ service_name }}`)" priority : 1 services : - name : "{{ service_name }}" port : "{{ service_port }}" weight : 1 Note This setup requires that both services have the same base path. Priority ï A higher priority value gives the route a higher priority.
A value of 0 means that default rules length sorting is used. Rule matchers ï To see a list of all available matchers, see: Rule Weight ï Weighted Round Robin is used to load balance
the requests between multiple services based on weights. Persistent storage ï Currently only in use in our oscommon clusters. Utilizes Rook with Ceph . Grow a block device ï For Prometheus we use block devices from Ceph. A block device is fixed in size and can only be used by one instance at a time. To grow an existing block device in Kubernetes we need to: Shutdown any pod using the PersistentVolumeClaim Determine what Ceph image is used for the PersistentVolume Grow the Ceph image Grow the filesystem (ext4/xfs) on the block device Start the pod again In the below example we will grow the disk used for Prometheus. Note For Prometheus, there are as many disks as there are Prometheus pods. Meaning you have to do all steps except shutdown/start for each disk. Get PVC name(s) for the pod(s) ï For a pod you can list the PersistentVolumeClaim names with for instance: namespace="metrics-acc"
kubectl -n $namespace get pods $podname -o jsonpath='{ ..claimName }' Set the variable pvc to the name of the PVC you wish to grow: pvc = "name-of-pvc" Shutdown Prometheus ï The Prometheus deployment is controlled by a CRD, edit it and change âreplicas: 2â to âreplicas: 0â: kubectl -n $namespace edit prometheus promstack Wait until the pods are gone before proceeding! Map PVC to Ceph image ï Find the PersistentVolume name with: pv = "$(kubectl -n $namespace get pvc $pvc -o jsonpath='{ .spec.volumeName }')" Now find the name of the Ceph image used for the PersistentVolume with: kubectl get pv $pv -o jsonpath='{ .spec.csi.volumeHandle }' | cut -d'-' -f6- Grow the Ceph image ï Log in to the Ceph dashboard for the correct oscommon cluster, for instance https://ceph-uv.common.sbab.se The username is âadminâ, retrieve the password with: kubectl - n rook - ceph get secret rook - ceph - dashboard - password - o jsonpath = "{['data']['password']}" | base64 -- decode && echo In the left-hand navigation go to âBlockâ -> âImagesâ. Copy the full image id/name from the previous step into the search bar to make sure you edit the correct image. Click the red âEditâ button at the top of the table view. Change the size to what you want and click âEdit RBDâ. Done. Grow the filesystem ï For this we will start a Kubernetes Job in a container that contains the necessary CLI tools, with the PersistentVolumeClaim mounted. Make sure to create the Job in the correct namespace and to set the name of PersistentVolumeClaim: --- apiVersion : batch / v1 kind : Job metadata : name : dresize spec : template : spec : containers : - name : debian image : debian command : - resize2fs - / dev / rbd0 volumeMounts : - name : volume - to - resize mountPath : / var / lib / datas - restore securityContext : privileged : true volumes : - name : volume - to - resize persistentVolumeClaim : claimName : prometheus - promstack - db - prometheus - promstack - 1 restartPolicy : Never When the Job is completed you can check the logs: kubectl -n $namespace logs job/dresize Then you can cleanup the Job pod with: kubectl -n $namespace delete job dresize Start Prometheus ï The Prometheus deployment is controlled by a CRD, edit it and change âreplicas: 0â to âreplicas: 2â: kubectl -n $namespace edit prometheus promstack External ï The external Kubernetes cluster is used for testing by our external partners, e.g. Skatteverket.
To deploy the cluster, use these variables when running the Deploy cluster playbook in ossys : primary_master = ext - m1 - ki parentgroup = ext - k8s - ki mastergroup = ext - k8smaster - ki workergroup = ext - k8sworker - ki proxygroup = ext - k8sproxy - ki Cheat sheet ï Tip For more information, see the official kubectl Cheat Sheet . Get non-running pods: kubectl get pods -- field - selector = status . phase != Running - A Remove non-running pods: kubectl delete pods -- field - selector = status . phase != Running - A Previous Next © Copyright 2024, TNT. Built with Sphinx using a theme provided by Read the Docs .