NVIDIA — SBAB Docs  documentation SBAB Docs Ansible Controller Ansible Roles Ansible Vault Argo Workflows Authentication & Authorization Bitbucket Boost Chatbot Booli Ceph Dex Fortigate troubleshooting Generate certificate Go GitHub Copilot GitLab @Sbab User Guide GitLab Maintenance Grafana Istio Java Jenkins Jenkinsfile Jmeter Kafka Maintenance Kafka @Sbab User Guide Kafka Connect Kubectl Access Kubernetes Lab environment Local Open Web Metrics infrastructure MongoDB Netbox NVIDIA NVIDIA driver installation Troubleshooting NVIDIA k8s setup License servers and network architecture License Administration Checking active licenses Getting a license Removing a license Neo4J Sonatype Nexus repository Oracle OWASP Database OWASP @Sbab User Guide Pact Principles of Security Prometheus ReactJS Redis Renovate S3 Security Guidelines for Developers Sentry Maintenance Sentry User Guide Configuring Variables for Services Deployed to Kubernetes Software Architecture SonarQube Maintenance SonarQube User Guide Structurizr System Landscape Vagrant Zipkin Windows Pipelines SBAB Docs » NVIDIA View page source NVIDIA ï NVIDIA driver installation ï Use the role nvidia to install the drivers, located at nvidia . You need an AlmaLinux instance deployed with a flavor beginning with G. . The role installs the drivers and automatically retrieves a license from the license servers, so you also need to connect your instances to the nvidiadls network. The way you can do this is by defining the variable os_nics on the host level in the inventory. Here is an example: os_nics : - net - name : swarm - net - name : nvidiadls If you need a floating ip make sure the variable intnetworkname is defined, the floating ip will NAT to the network defined in intnetworkname . If intnetworkname is not defined the floating_ip task wonât know which of the two networks to NAT the floating ip to. Troubleshooting ï After installation is done you should see in the logs of the service nvidia-gridd.service that it has successfully aquired a license. The configuration of the gridd service is extremely simple we only set one value FeatureType=4 set in /etc/nvidia/gridd.conf Useful commands: nvidia - smi lspci dmesg journalctl - u nvidia - gridd . service - ef nvidia-smi is how you get information from the GPU itself, it can show you if the license is working correctly as well. lspci Show all attached PCI devices, and should show a Nvidia GPU. If there are non showing up, make sure you deployed your machine with the correct flavor. The flavor makes it so the VM is provisioned on a Openstack Compute instance with a GPU. If the flavor is incorrect the VM will be provisioned on a non GPU Compute host. dmesg Shows device driver messages from the kernel. You can also look at the current kernel messages at /var/log/messages To check the token license file you can use this command: cut - d "." - f1 , 2 snovit - dls - client . tok | sed 's/\./ \n /g' | base64 - d | jq - r '.' NVIDIA k8s setup ï In order to consume the GPU with a container we have to use NVIDIAâs container runtime. Luckily itâs just runc (which we already use) with some prehooks so we can add the nvidia-container-runtime to CRI-O as an extra runtime. The nvidia role installs the required runtime binaries. You just have to configure an extra runtime in CRI-O by specifiying in your hosts variables, and run the crio role: crio_extra_runtimes : - name : nvidia - container - runtime path : "/bin/nvidia-container-runtime" root : "/run/nvidia_runc" You can now create a runtimeclass in your kubernetes cluster for your containers to use: apiVersion : node . k8s . io / v1 kind : RuntimeClass metadata : name : nvidia - container - runtime handler : nvidia - container - runtime And lastly add this to your pod spec: ... spec : runtimeClassName : nvidia - container - runtime License servers and network architecture ï There are two license servers in each data center and 1 admin server per data center. In each datacenter there is a network named nvidiadls that is shared between the tentants (environments). To get access to the license servers from your VM you also need security group sg_nvidiadls_client which allow traffic to the license servers. In both datacenters the license servers has IP 10.207.0.21 and 10.207.0.22 . License Administration ï Checking active licenses ï We have 8 licenses per datacenter. The licenses are shared between the tentants (environments) in each datacenter. To access the License servers admin webgui you need to create a proxy trough the admin servers, follow these steps: (On Mac) Forward a Socket proxy to the admin server in the for the datacenter you need to reach. SSH command to open a SOCKS proxy to the KI datacenter: ssh -TD 1337 ansible@nvidia-adm1-ki.common.sbab.se (On Mac) Open a new terminal and open a new Chrome window with proxy settings with the newly created SOCKS proxy: /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --proxy-server="socks5://127.0.0.1:1337" --host-resolver-rules="MAP * ~NOTFOUND , EXCLUDE 127.0.0.1" --user-data-dir=test --no-first-run --no-default-browser-check The license servers in KI datacenter has the same IP address as in UV datacenter. Surf to one of the license servers in your new Chrome window: License Server 1 License Server 2 enter âthisisunsafeâ in the chrome window to skip the ssl warnings. The password is in Vault. prod/openstack/nvidia/ dls_admin Getting a license ï The nvidia role will automatically retrieve a license. It installs a client token (JWT) that is used to get a license. The role also sets two lines in the /etc/hosts file, since the it tries to retrieve license from servers defined in in the license token: host - 10 - 207 - 0 - 21. openstacklocal 10.207.0.21 host - 10 - 207 - 0 - 22. openstacklocal 10.207.0.22 Removing a license ï To gracefully remove a licensed GPU VM you can stop the service nvidia-gridd.service then wait for about 10 minutes. The license will be automatically unregistered from the license servers. However if the server has been removed without the service being stopped then the license will still be active on the license servers and will NOT be automatically removed. You then need to manually remove the license from the administration webgui on the license servers. There can be several licenses for a single hostname so be careful when you unregister a license. Check IP of the license and your VM and make sure they match. Previous Next © Copyright 2024, TNT. Built with Sphinx using a theme provided by Read the Docs .