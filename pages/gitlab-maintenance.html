GitLab Maintenance — SBAB Docs  documentation SBAB Docs Ansible Controller Ansible Roles Ansible Vault Argo Workflows Authentication & Authorization Bitbucket Boost Chatbot Booli Ceph Dex Fortigate troubleshooting Generate certificate Go GitHub Copilot GitLab @Sbab User Guide GitLab Maintenance GitLab K8S cluster Node roles and specs Cluster size Cluster design Node anti-affinity & scaling Node labels/selectors Ansible groups Ansible group variables Deploying the K8S cluster Deploy cluster Rook Ceph persistent storage PostgreSQL Operator Elastic Cloud on Kubernetes Sensitive Secrets License Gitlab stack Overview PostgreSQL cluster Deployment Removal Redis Sentinel cluster Deployment Removal Elasticsearch cluster Deployment Removal Indexing Gitlab Helm chart Version Deployment Removal Docker Runner Deployment Tasks Maintenance mode Backups Create new backup Scale down old Gitlab instance Find backup to restore Restore backup Workaround PostgreSQL issue Migration Grafana Troubleshooting Connectivity issues ElasticSearch indexing problem Upgrade gitlab-rake Grafana Istio Java Jenkins Jenkinsfile Jmeter Kafka Maintenance Kafka @Sbab User Guide Kafka Connect Kubectl Access Kubernetes Lab environment Local Open Web Metrics infrastructure MongoDB Netbox NVIDIA Neo4J Sonatype Nexus repository Oracle OWASP Database OWASP @Sbab User Guide Pact Principles of Security Prometheus ReactJS Redis Renovate S3 Security Guidelines for Developers Sentry Maintenance Sentry User Guide Configuring Variables for Services Deployed to Kubernetes Software Architecture SonarQube Maintenance SonarQube User Guide Structurizr System Landscape Vagrant Zipkin Windows Pipelines SBAB Docs » GitLab Maintenance View page source GitLab Maintenance ï GitLab K8S cluster ï Node roles and specs ï There are five types of servers (nodes) in the Gitlab K8S cluster, with different roles and system resources in line with those roles. Proxy - 1 core, 4G RAM - Primary function is to load balance access to the Kubernetes API across all masters Master - 2 core, 4G RAM - Runs the control plane components for the K8S cluster, no other container workloads Ceph - 4 core, 12G RAM - Runs most of the container workloads for the Rook Ceph cluster, providing persistent storage to the K8S cluster. These servers have an extra OpenStack Cinder volume attached to be used for Ceph. Gitlab - 4 core, 12G RAM - Runs most of the container workloads for the Gitlab instance including PostgreSQL, Redis Worker - 4 core, 12G RAM - Runs all general purpose container workloads in the cluster, mostly all the CI/CD jobs from the Gitlab Runner(s) The above specifications are based on data from our other K8S clusters, specs of the current Jenkins build slaves and the recommendations for the Gitlab Helm Chart. Cluster size ï 2x Proxy 3x Master 6x Ceph 3x Gitlab 3x Worker Grand total of 17 VMs. Cluster design ï Node anti-affinity & scaling ï Each type of node is configured with anti-affinity in OpenStack to ensure even spread across the underlying hypervisors. If we need to increase capacity in the future it should primarily be done by vertically scaling the VM instances, so we donât try to deploy more nodes than âfitâ in an anti-affinity group. Node labels/selectors ï Ceph, Gitlab and Worker nodes are labeled in K8S. We should use node selectors when deploying container workloads to ensure they are distributed to the correct nodes whenever possible. The labels are: sbab.se/role=ceph sbab.se/role=gitlab sbab.se/role=worker Ansible groups ï There are a lot of groups and you need to understand inheritance if you are going to be changing group variables. gitlab - top group, includes all nodes gitlab_ki - all nodes in Kista gitlab_uv - all nodes in Upplands-VÃ¤sby gitlab_k8s - all nodes except Proxy, ie. all nodes in the K8S cluster gitlab_k8s_master - all Master nodes gitlab_k8s_master_ki - Master nodes in Kista gitlab_k8s_master_uv - Master nodes in Upplands-VÃ¤sby gitlab_k8s_nonmaster - all Ceph/Gitlab/Worker nodes gitlab_k8s_nonmaster_ki - Ceph/Gitlab/Worker nodes in Kista gitlab_k8s_nonmaster_uv - Ceph/Gitlab/Worker nodes in Upplands-VÃ¤sby gitlab_k8s_ceph - all Ceph nodes gitlab_k8s_ceph_ki - Ceph nodes in Kista gitlab_k8s_ceph_uv - Ceph nodes in Upplands-VÃ¤sby gitlab_k8s_gitlab - all Gitlab nodes gitlab_k8s_gitlab_ki - Gitlab nodes in Kista gitlab_k8s_gitlab_uv - Gitlab nodes in Upplands-VÃ¤sby gitlab_k8s_worker - all Worker nodes gitlab_k8s_worker_ki - Worker nodes in Kista gitlab_k8s_worker_uv - Worker nodes in Upplands-VÃ¤sby gitlab_proxy - all Proxy nodes gitlab_proxy_ki - Proxy nodes in Kista gitlab_proxy_uv - Proxy nodes in Upplands-VÃ¤sby Ansible group variables ï Obviously try to keep things simple, donât set the same variable+value in multiple places. At the time of this writing group variables are defined for: gitlab gitlab_ki gitlab_uv gitlab_k8s gitlab_k8s_ceph gitlab_k8s_gitlab gitlab_k8s_master gitlab_k8s_worker gitlab_proxy Deploying the K8S cluster ï Deploy cluster ï Extra variables to use with all playbooks : primary_master = glb-m2-ki parentgroup = gitlab_ki mastergroup = gitlab_k8s_master_ki workergroup = gitlab_k8s_nonmaster_ki proxygroup = gitlab_proxy_ki Ansible inventory to use is oscommon . First of all run the k8s/full_deployment.yml playbook to deploy the Kubernetes cluster. For KI datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/full_deployment.yml -e primary_master = glb-m2-ki -e parentgroup = gitlab_ki -e mastergroup = gitlab_k8s_master_ki -e workergroup = gitlab_k8s_nonmaster_ki -e proxygroup = gitlab_proxy_ki For UV datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/full_deployment.yml -e primary_master = glb-m2-uv -e parentgroup = gitlab_uv -e mastergroup = gitlab_k8s_master_uv -e workergroup = gitlab_k8s_nonmaster_uv -e proxygroup = gitlab_proxy_uv Then move on with additional dependencies below. Rook Ceph persistent storage ï Run the playbook k8s/cluster_services/k8s_cluster_storage_rook.yml For KI datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/cluster_services/k8s_cluster_storage_rook.yml -e primary_master = glb-m2-ki -e parentgroup = gitlab_ki -e mastergroup = gitlab_k8s_master_ki -e workergroup = gitlab_k8s_nonmaster_ki -e proxygroup = gitlab_proxy_ki For UV datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/cluster_services/k8s_cluster_storage_rook.yml -e primary_master = glb-m2-uv -e parentgroup = gitlab_uv -e mastergroup = gitlab_k8s_master_uv -e workergroup = gitlab_k8s_nonmaster_uv -e proxygroup = gitlab_proxy_uv The playbook will complete quickly, do not proceed with other playbooks until you have verified that the Ceph cluster is up and running: kubectl -n rook-ceph get cephcluster Once completed you will have âCluster created successfullyâ in the output: NAME        DATADIRHOSTPATH   MONCOUNT   AGE   PHASE   MESSAGE                        HEALTH rook-ceph   /var/lib/rook     3          19h   Ready   Cluster created successfully   HEALTH_OK If you want detailed information: kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status PostgreSQL Operator ï Run the playbook k8s/cluster_services/k8s_cluster_rdb_postgresql.yml For KI datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/cluster_services/k8s_cluster_rdb_postgresql.yml -e primary_master = glb-m2-ki -e parentgroup = gitlab_ki -e mastergroup = gitlab_k8s_master_ki -e workergroup = gitlab_k8s_nonmaster_ki -e proxygroup = gitlab_proxy_ki For UV datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/cluster_services/k8s_cluster_rdb_postgresql.yml -e primary_master = glb-m2-uv -e parentgroup = gitlab_uv -e mastergroup = gitlab_k8s_master_uv -e workergroup = gitlab_k8s_nonmaster_uv -e proxygroup = gitlab_proxy_uv The playbook will complete almost immediately, then you can check the progress/status with: kubectl -n postgres-operator get deployment Once it is ready, the output should be something similar to: NAME                READY   UP-TO-DATE   AVAILABLE   AGE postgres-operator   1/1     1            1           25h s3cmd-backups       1/1     1            1           25h Elastic Cloud on Kubernetes ï Run the playbook k8s/cluster_services/k8s_cluster_eck_operator.yml to install the operator for Elastic Cloud on Kubernetes. For KI datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/cluster_services/k8s_cluster_eck_operator.yml -e primary_master = glb-m2-ki -e parentgroup = gitlab_ki -e mastergroup = gitlab_k8s_master_ki -e workergroup = gitlab_k8s_nonmaster_ki -e proxygroup = gitlab_proxy_ki For UV datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/cluster_services/k8s_cluster_eck_operator.yml -e primary_master = glb-m2-uv -e parentgroup = gitlab_uv -e mastergroup = gitlab_k8s_master_uv -e workergroup = gitlab_k8s_nonmaster_uv -e proxygroup = gitlab_proxy_uv Check status with: kubectl -n eck-operator get statefulset NAME               READY   AGE elastic-operator   1/1     2d2h Sensitive Secrets ï To update the following environment variables: ANSIBLE_PROD_PASSWORD prod/ansible/ansible_vault_passwd ANSIBLE_HASHI_VAULT_ROLE_ID common/gitlab/ANSIBLE_HASHI_VAULT_ROLE_ID ANSIBLE_HASHI_VAULT_SECRET_ID common/gitlab/ANSIBLE_HASHI_VAULT_SECRET_ID Follow these steps: Encode the password locally: echo "{the password}" | base64 Navigate to https://gitlab.common.sbab.se/admin/application_settings/ci_cd and update the password License ï The license is stored in https://vault.sbab.se/ui/vault/secrets/common/show/gitlab/premium-license and can be uploaded in GitLab at: https://gitlab.sbab.se/admin/license Gitlab stack ï Overview ï This document is about the Gitlab application stack and how to deploy it and perform various administrative tasks. At the top level there is essentially four parts to deploy once you have a Kubernetes cluster up and running with Rook Ceph and the Zalando Postgres Operator: PostgreSQL cluster Redis Sentinel cluster Elasticsearch cluster The Gitlab Helm chart The Gitlab Docker runner PostgreSQL cluster ï Deployment ï Run the playbook gitlab/postgresql_cluster.yml with the oscommon inventory and the extra variables for Kubernetes: For KI datacenter: ansible-playbook --inventory inventories/oscommon playbooks/gitlab/postgresql_cluster.yml -e primary_master = glb-m2-ki -e parentgroup = gitlab_ki -e mastergroup = gitlab_k8s_master_ki -e workergroup = gitlab_k8s_nonmaster_ki -e proxygroup = gitlab_proxy_ki For UV datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/gitlab/postgresql_cluster.yml -e primary_master = glb-m2-uv -e parentgroup = gitlab_uv -e mastergroup = gitlab_k8s_master_uv -e workergroup = gitlab_k8s_nonmaster_uv -e proxygroup = gitlab_proxy_uv The playbook will complete almost instantly as it only creates the PostgreSQL cluster specification which is then picked up by the Postgres Operator and deployed. To see progress/state: kubectl -n gitlab-postgres get pods
kubectl -n gitlab-postgres describe postgresql tnt-gitlab Removal ï Run the below from a master in the Kubernetes cluster: kubectl -n gitlab-postgres delete postgresql tnt-gitlab If you want to also remove the namespace gitlab-postgres , please wait a few minutes and make sure the pods and the PersistentVolumeClaim(s) are gone before deleting the namespace. Redis Sentinel cluster ï Deployment ï Run the playbook gitlab/redis.yml with the oscommon inventory and the extra variables for Kubernetes: For KI datacenter: ansible-playbook --inventory inventories/oscommon playbooks/gitlab/redis.yml -e primary_master = glb-m2-ki -e parentgroup = gitlab_ki -e mastergroup = gitlab_k8s_master_ki -e workergroup = gitlab_k8s_nonmaster_ki -e proxygroup = gitlab_proxy_ki For UV datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/gitlab/redis.yml -e primary_master = glb-m2-uv -e parentgroup = gitlab_uv -e mastergroup = gitlab_k8s_master_uv -e workergroup = gitlab_k8s_nonmaster_uv -e proxygroup = gitlab_proxy_uv This deploys a Helm chart and waits until all pods are running so it can take 5-10min with a fresh deployment. You need to check the logs from Sentinel in each instance to verify that the cluster is properly connected to all instances. Each Sentinel instance should show connections to two other Sentinel instances: kubectl -n gitlab-redis logs glb-redis-node-0 -c sentinel ( .. ) 1 :X 24 Feb 2021 16 :05:01.134 * +sentinel sentinel 23a016f94cb02f2d13b391287d5e14aed4ff80df 172 .18.0.5 26379 @ mymaster 172 .18.10.3 6379 1 :X 24 Feb 2021 16 :05:15.337 * +sentinel sentinel 409c3816daae3975c9e723e4305803d204d5d355 172 .18.5.133 26379 @ mymaster 172 .18.10.3 6379 ( .. ) If a pod is not showing two Sentinel connection while all three pods are in Running state, try deleting the pod that is having issue and see if the ânewâ pod works. You can verify the Sentinel master through the following command with redis-cli towards the local instance, $Â kubectl -n gitlab-redis exec -it glb-redis-node-1 -- redis-cli -p 26379 172 .18.10.3:26379> sentinel master mymaster 1 ) "name" 2 ) "mymaster" 3 ) "ip" 4 ) "172.18.10.3" ... If you get an error that you are not able to connect, for example âNOAUTH Authentication requiredâ, ensure that you have authentication turned off in the âgitlab_redis.ymlâ configuration, inventories/oscommon/group_vars/gitlab_k8s/gitlab_redis.yml .
Verify this against the helm chart . More info regarding Sentinel is available at redis.io . Removal ï As this is a installed Helm chart you want to uninstall it properly by running the below from a Kubernetes master: helm -n gitlab-redis uninstall glb To completely remove all resources it is also a good idea to delete the namespace, but wait a minute or two for actions from the helm uninstall to complete. Elasticsearch cluster ï Deployment ï Run the playbook gitlab/elasticsearch_cluster.yml with the oscommon inventory and the extra variables for Kubernetes: For KI datacenter: ansible-playbook --inventory inventories/oscommon playbooks/gitlab/elasticsearch_cluster.yml -e primary_master = glb-m2-ki -e parentgroup = gitlab_ki -e mastergroup = gitlab_k8s_master_ki -e workergroup = gitlab_k8s_nonmaster_ki -e proxygroup = gitlab_proxy_ki For UV datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/gitlab/elasticsearch_cluster.yml -e primary_master = glb-m2-uv -e parentgroup = gitlab_uv -e mastergroup = gitlab_k8s_master_uv -e workergroup = gitlab_k8s_nonmaster_uv -e proxygroup = gitlab_proxy_uv This deploys a definition of a Elasticsearch cluster. The definition is processed by the operator for Elastic Cloud on Kubernetes. The playbook should wait until the Elasticsearch cluster is up and running. You can check the status yourself with: $ kubectl -n gitlab-elasticsearch get elasticsearch
NAME   HEALTH   NODES   VERSION   PHASE   AGE
es     green 3 7 .15.1    Ready   24h Removal ï Delete the cluster definition: kubectl -n gitlab-elasticsearch delete elasticsearch es The operator for Elastic Cloud on Kubernetes will handle removal of the pods etc. Iâm not sure if it also removes the PersistentVolumeClaim(s), wait for the pods to be deleted then you can check if there are still any PersistentVolumeClaim(s) left to delete: kubectl -n gitlab-elasticsearch get pvc Indexing ï To manually force trigger the indexing of all projects: https://gitlab.sbab.se/admin/application_settings/advanced_search Gitlab Helm chart ï Version ï Make sure to set gitlab_chart_version , see this version matrix . Deployment ï Run the playbook gitlab/gitlab.yml with the oscommon inventory and the extra variables for Kubernetes: For KI datacenter: ansible-playbook --inventory inventories/oscommon playbooks/gitlab/gitlab.yml -e primary_master = glb-m2-ki -e parentgroup = gitlab_ki -e mastergroup = gitlab_k8s_master_ki -e workergroup = gitlab_k8s_nonmaster_ki -e proxygroup = gitlab_proxy_ki For UV datacenter: ansible-playbook --inventory inventories/oscommon playbooks/k8s/gitlab/gitlab.yml -e primary_master = glb-m2-uv -e parentgroup = gitlab_uv -e mastergroup = gitlab_k8s_master_uv -e workergroup = gitlab_k8s_nonmaster_uv -e proxygroup = gitlab_proxy_uv This creates some prereq resources in Kubernetes and then installs the Gitlab Helm chart. The playbook will wait for the pods belonging to the Gitlab Helm chart to enter Running state. It is a good idea to keep an eye on how the pods are doing while waiting for the chart installation to complete: kubectl -n gitlab get pods Especially keep an eye on the glb-migrations pod, it should end up in âCompletedâ state, usually takes about 5min. Any other state is bad as that pod handles database setup and similar tasks. Removal ï As this is a installed Helm chart you want to uninstall it properly by running the below from a Kubernetes master: helm -n gitlab uninstall glb To completely remove all resources it is also a good idea to delete the namespace, but wait 10min or so for actions from the helm uninstall to complete. Note that these actions will delete the PersistentVolumes but not the âS3â Buckets in Ceph. Docker Runner ï Deployment ï Run the playbook gitlab/gitlab_runner_docker.yml with the oscommon inventory, force_registration=true and the extra variables for Kubernetes: For KI datacenter: ansible-playbook -i inventories/oscommon playbooks/gitlab/gitlab_runner_docker.yml -e force_registration = true -e primary_master = glb-m2-ki -e parentgroup = gitlab_ki -e mastergroup = gitlab_k8s_master_ki -e workergroup = gitlab_k8s_nonmaster_ki -e proxygroup = gitlab_proxy_ki For UV datacenter: ansible-playbook -i inventories/oscommon playbooks/gitlab/gitlab_runner_docker.yml -e force_registration = true -e primary_master = glb-m2-uv -e parentgroup = gitlab_uv -e mastergroup = gitlab_k8s_master_uv -e workergroup = gitlab_k8s_nonmaster_uv -e proxygroup = gitlab_proxy_uv Tasks ï Maintenance mode ï Gitlab can be set to run in Maintenance mode (Read Only-mode) as a way of preventing data loss during various operations tasks, e.g when running the backup-utility. The two preferred ways of enabling/disabling Maintenance mode are: Through the Rails console which can be located in the toolbox pod. Exec into the toolbox pod: kubectl -n gitlab exec -it deploy/glb-toolbox -c toolbox -- bash Run the rails command for enabling (use false for disabling) maintenance mode gitlab-rails console # Wait a little bit ::Gitlab::CurrentSettings.update! ( maintenance_mode: true ) # Wait even more Through the ui , Be advised that once Gitlab is running in maintenance mode, login is treated as write operation as a consequence of our authentication setup. Backups ï Backups are managed by Gitlab, as described in the official documentation here and here . Gitlab is configured to create a daily tgz archive at 02:30 in the morning, this is uploaded to S3 bucket gitlab-backup-storage in the S3 service provided by techops. The S3 bucket gitlab-backup-storage is configured with a retention of 7 days. The backups runs as a Kubernetes CronJob so you can check on the history by running: kubectl -n gitlab get job # Example of a job: # glb-task-runner-backup-1616121000 kubectl -n gitlab logs glb-task-runner-backup-1616121000 Additionally there is pod preconfigured with access to the S3 bucket containing the backups: kubectl -n gitlab exec -it deploy/s3cmd-backups -- bash

s3cmd ls 2021 -03-10 13 :32  s3://gitlab-backup-storage 2021 -03-10 13 :32  s3://gitlab-tmp-storage Create new backup ï If you need to perform a backup follow these steps: Locate the task runner: kubectl -n gitlab get pods -lrelease = glb,app = task-runner Create the backup: kubectl -n gitlab exec <Task Runner pod name> -it -- backup-utility --skip artifacts For more information, see: https://docs.gitlab.com/charts/backup-restore/backup.html Scale down old Gitlab instance ï To ensure no traffic reaches the old Gitlab instance please scale down Gitaly and the web interface before continuing: kubectl -n gitlab scale deploy glb-webservice-default --replicas = 0 kubectl -n gitlab scale statefulset glb-gitaly --replicas = 0 kubectl -n gitlab scale deploy glb-gitlab-shell --replicas = 0 Find backup to restore ï Use the task-runner pod to browse the S3 bucket: kubectl -n gitlab exec deploy/glb-task-runner -- s3cmd ls s3://gitlab-backup-storage Backups will be named similar to: 1648138524_2022_03_24_14.0.0-ee_gitlab_backup.tar Check that gitlab_chart_version match the Gitlab version of the backups (see above example). See this version matrix . To get the Gitlab version of the backup: $ echo " $BACKUP_FILENAME " | | cut -d '_' -f5 14 .0.0-ee To get the argument to use for the backup-utility: $ echo " $BACKUP_FILENAME " | sed 's/^.* //g' | cut -d '_' -f1-5
1648138524_2022_03_24_14.0.0-ee Restore backup ï kubectl -n gitlab exec -it deploy/glb-task-runner -- backup-utility --restore -t 1616121012_2021_03_19_13.9.4-ee Workaround PostgreSQL issue ï Known issue . Workaround is to connect to the PostgreSQL database and execute: kubectl -n gitlab-postgres exec -it deploy/psql -- psql gitlabhq_production

DROP EXTENSION pg_stat_statements CASCADE ; Migration ï Create new cluster in the target datacenter Create Rook Ceph persistent storage Create PostgreSQL Operator Create Elastic Cloud on Kubernetes Create PostgreSQL cluster Create Redis Sentinel cluster Create Elasticsearch cluster Enable Gitlab Maintenance mode Create a db back up (active cluster) Scaled down services in active cluster Run GitLab helm chart . Workaround PostgreSQL issue Restore backup Run GitLab helm chart . If failed run it again. Index all projects Install docker runners Create and restore OWASP DB . Checkout owasp-db repo on ansible-controller and set doRestoreFromBackup: true initializeDatabase: true For KI datacenter: ansible-playbook -i inventories/oscommon playbooks/deploy-k8s-helm-chart.yml -e "k8smaster=glb-m2-ki service_type=tools service_name=owasp-db-650 service_namespace=owasp-db-650 workspace_dir=/tmp/owasp-db" For UV datacenter: ansible-playbook -i inventories/oscommon playbooks/deploy-k8s-helm-chart.yml -e "k8smaster=glb-m2-uv service_type=tools service_name=owasp-db-650 service_namespace=owasp-db-650 workspace_dir=/tmp/owasp-db" Test run some pipelines and check if all gitlab-runners run in new GitLab instance Clean up Remove all glb-* nodes from OpenStack Remove all cepf disks from Openstack Grafana ï Username/Password: Default admin credentials KI: https://grafana-internal-gitlab-ki.common.sbab.se/ UV: https://grafana-internal-gitlab-uv.common.sbab.se/ Troubleshooting ï Connectivity issues ï We are occasionally seeing issues in the Gitlab K8S cluster. First indication is usually that DNS lookups start failing with these logs: âconnect: no route to hostâ host=glb*.common First step is to identify on which workers (and their IPâs) the coredns pods are running on: kubectl -n kube-system get pod -o wide | grep coredns We can then use Emilâs pod to see if we get a quick answer or if get stuck. We can run this for each worker IP: kubectl -n gitlab exec deploy/emildns -- dig @ ${ WORKER_IP_ADDRESS } sbab.se We can then drain the node that does not answer: kubectl drain ${ NODE_NAME } --ignore-daemonsets --delete-local-data This buys us time to continue our troubleshooting or in worst case restart/recreate the node. ElasticSearch indexing problem ï Below you can find the steps to reset ElasticSearch indexing Start a shell in the task runner pod kubectl -n gitlab exec -it glb-task-runner-<pod id> -- bash Run gitlab-rake gitlab:elastic:index Upgrade ï Run the gitlab/migrate.yml playbook. For more information, see Migration Run the gitlab/gitlab.yml playbook with -e gitlab_chart_version=<new version> Add -e gitlab_host=gitlab-<dc>.sbab.se if you want to test pipelines Check the logs of the toolbox migration pod gitlab-rake ï kubectl - n gitlab exec deploy / glb - toolbox - c toolbox -- gitlab - rake < command > Previous Next © Copyright 2024, TNT. Built with Sphinx using a theme provided by Read the Docs .