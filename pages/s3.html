S3 — SBAB Docs  documentation SBAB Docs Ansible Controller Ansible Roles Ansible Vault Argo Workflows Authentication & Authorization Bitbucket Boost Chatbot Booli Ceph Dex Fortigate troubleshooting Generate certificate Go GitHub Copilot GitLab @Sbab User Guide GitLab Maintenance Grafana Istio Java Jenkins Jenkinsfile Jmeter Kafka Maintenance Kafka @Sbab User Guide Kafka Connect Kubectl Access Kubernetes Lab environment Local Open Web Metrics infrastructure MongoDB Netbox NVIDIA Neo4J Sonatype Nexus repository Oracle OWASP Database OWASP @Sbab User Guide Pact Principles of Security Prometheus ReactJS Redis Renovate S3 S3 End-User Documentation Gettings access TLDR Connecting with CLI Administration guide Users & Buckets management Deployment Create buckets Troubleshooting Cheat sheet Check current primary cluster Check sync status between clusters Pull realm (If period is missing commits or joining new cluster) Fail over to second cluster If old master is recovered simply run this again Security Guidelines for Developers Sentry Maintenance Sentry User Guide Configuring Variables for Services Deployed to Kubernetes Software Architecture SonarQube Maintenance SonarQube User Guide Structurizr System Landscape Vagrant Zipkin Windows Pipelines SBAB Docs » S3 View page source S3 ï S3 End-User Documentation ï S3 is deployed on two Kubernetes clusters with a Ceph storage backend. Ceph provides an S3 service through their Rados Gateway (RGW). For redundancy we use multi-site configuration to sync the data and buckets between the clusters.Â  The clusters are in active-active mode, so both clusters can be read and written from. To access S3 there are 1 endpoint per cluster. Since both are active at all time you can use both interchangeably . We are looking into load-balance these two endpoints so we get one endpoint for our end-users. This is not implemented yet. https://s3-sbab-uv.common.sbab.se Â (Master) https://s3-sbab-ki.common.sbab.se Â (Slave) Gettings access ï You need access rights to able to access. Request can be made with Linux Ops team to get a bucket or user created with access rights. Ceph S3 implentation has full S3-compatible support for canned ACL (Grants). When getting access to a S3 bucket you will be receive an Access Key and a Secret Key to authenticate with. When you request only a bucket this user will have a limit of 1 bucket, that capability can be changed but requires some extra steps from the Linux Ops team. Submit a request for a user and subusers if you require more than one bucket and more users capabilities: Name of user, Names of subusers (not required) Quota of number of buckets. Max total size of buckets together Max number of objects. This kind of user we create can only create buckets in its own space and canât access other users buckets. However we can create a subuser that is linked with the main user. The subuser can have itâs own quotas and ACL (read, write, readwrite, full), and can be used in Bucket policies and roles. If you want a more comprehensive understanding of what is possible to configure on our S3, have look at this documentation: https://docs.ceph.com/en/latest/radosgw/ . Topics that can be of interest: Roles , S3 API ,Â Bucket Policies TLDR Connecting with CLI ï Using AWS CLI Create a configuration profile. Replace with a profile name of your liking. Aws s3 cli does not include your computers installed CA certificates so you need to get it and specify a path to it: $ aws configure --profile=<profile> set aws_access_key_id <access key>
$ aws configure --profile=<profile> set aws_secret_access_key <secret access key>
$ aws configure --profile=<profile> set region uv
$ aws configure --profile=<profile> set ca_bundle <SBABBankRootCA1 path> Now you can use the aws s3 cli. This will list your buckets, you can change s3-sbab-uv to s3-sbab-ki interchangeably: $ aws --profile=<profile> --endpoint=https://s3-sbab-uv.common.sbab.se s3 ls Using mc (Minio) client With Minio MC you create a conection alias like with mc alias: $ mc alias set <ALIAS> https://s3-sbab-uv.common.sbab.se <YOUR-ACCESS-KEY> <YOUR-SECRET-KEY> --api S3v4 Administration guide ï Users & Buckets management ï When deploying new buckets or users always deploy to the primary cluster. Default is uv. Rook-ceph deploys two CRD:s for managing buckets and objectstore users. CephObjectStoreUser and CephObjectBucketClaim , the CephObjectBucketClaim will create a bucket and a objectstore user that can connect to that bucket. If you create a user that user will have no buckets created for itself and will have to create his/her own buckets. At the moment we deploy all buckets and users to the rook-ceph namespace. Buckets and users are configured in: inventories/oscommon/group_vars/ceph_k8s/rook/rook_buckets_and_users.yml Deployment ï ansible - playbook - i inventories / oscommon playbooks / s3 - sbab / k8s_cluster_rook_buckets_users . yml - e 'primary_cluster_master=ceph-k8s-m1-uv' Create buckets ï aws -- profile =< profile > -- endpoint = https : // s3 - sbab - uv . common . sbab . se s3api create - bucket -- bucket < bucket name > Troubleshooting ï Normal error is that cluster IP:s are defined under .endpoints in zonegroup config. To remove this you need to change the zone, and zonegroup config. If the dashboard canât show the object store and users you need to reconfigure with the ceph command. There is an internal user that does not exist as a Kubernetes definition. The name is something like rook-system-<store-name> . The acces-key and secret-key for that user need to be exported as files. Check for that user and reconfigure the dashboard with the following commands: ceph dashboard set - rgw - api - user - id < rook - system - user > ceph dashboard set - rgw - api - access - key - i < access - key - file > ceph dashboard set - rgw - api - secret - key - i < secret - key - file > Reminder : Always commit changes to period with radosgw-admin period update --commit Cheat sheet ï You can enter the toolbox pod interactively if you need interact with the ceph cluster administration tools: kubectl - n rook - ceph exec deploy / rook - ceph - tools - it -- bash Or you can prefix all Ceph and rados commands with: kubectl - n rook - ceph exec deploy / rook - ceph - tools - it -- < rook - ceph - tools command > Check current primary cluster ï radosgw - admin zonegroup get Check sync status between clusters ï radosgw - admin sync status Pull realm (If period is missing commits or joining new cluster) ï radosgw - admin realm pull -- url = "$ {PRIMARY_RGW_ENDPOINT} " -- access - key = "$ {ACCESS_KEY} " -- secret = "$ {SECRET_KEY} " Fail over to second cluster ï Make sure the first cluster is DOWN. You might need to restart the objectgateway pod (rook-ceph-rgw-sbab-s3) radosgw - admin zone modify -- rgw - zone = "$ {SECONDARY_ZONE} " -- master -- default radosgw - admin period update -- commit If old master is recovered simply run this again ï radosgw - admin zone modify -- rgw - zone = "$ {RECOVERED_PRIMARY_ZONE} " -- master -- default radosgw - admin period update -- commit Previous Next © Copyright 2024, TNT. Built with Sphinx using a theme provided by Read the Docs .